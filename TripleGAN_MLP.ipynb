{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNonmqqbbJrPI6HkxV/TMeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivahidi75/Semi-supervised/blob/main/TripleGAN_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mt\n",
        "import matplotlib.pyplot as pt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics         import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "from keras import layers as lys\n",
        "from keras import models as mls\n",
        "from keras import initializers\n",
        "from keras.utils import to_categorical as ct\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "import io\n",
        "import gdown\n",
        "import scipy.io"
      ],
      "metadata": {
        "id": "XSdcsjuUhKH9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gVrGQ0qQ-6oA"
      },
      "outputs": [],
      "source": [
        "class TripleGAN(tf.keras.Model):\n",
        "    def __init__(self, generator, discriminator, classifier, latent_dim):\n",
        "        super(TripleGAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.classifier = classifier\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, c_optimizer, g_loss_fn, d_loss_fn, c_loss_fn):\n",
        "        super(TripleGAN, self).compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.c_optimizer = c_optimizer\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.c_loss_fn = c_loss_fn\n",
        "\n",
        "    def train_step(self, sup_images, labels, unsup_images):\n",
        "        #real_images, real_labels = data  # Assuming data is a tuple of (images, labels)\n",
        "        real_images = tf.concat([sup_images, unsup_images], axis=0)\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # Generate fake images\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Labels for real and fake images\n",
        "        real_labels_disc = tf.ones((batch_size, 1))\n",
        "        fake_labels_disc = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            real_predictions = self.discriminator(real_images)\n",
        "            fake_predictions = self.discriminator(generated_images)\n",
        "            d_loss = self.d_loss_fn(real_labels_disc, real_predictions) + self.d_loss_fn(fake_labels_disc, fake_predictions)\n",
        "\n",
        "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_weights))\n",
        "\n",
        "        # Train the generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            generated_images = self.generator(random_latent_vectors)\n",
        "            fake_predictions = self.discriminator(generated_images)\n",
        "            g_loss = self.g_loss_fn(real_labels_disc, fake_predictions)\n",
        "\n",
        "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_weights))\n",
        "\n",
        "        # Train the classifier\n",
        "        with tf.GradientTape() as c_tape:\n",
        "            # Classify real labeled data\n",
        "            real_classifications = self.classifier(sup_images)\n",
        "            c_loss = self.c_loss_fn(labels, real_classifications)\n",
        "\n",
        "        c_gradients = c_tape.gradient(c_loss, self.classifier.trainable_weights)\n",
        "        self.c_optimizer.apply_gradients(zip(c_gradients, self.classifier.trainable_weights))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"c_loss\": c_loss}\n",
        "\n",
        "    def fit(self, labeled_dataset, unlabeled_dataset, epochs, callbacks=None):\n",
        "      combined_dataset = tf.data.Dataset.zip((labeled_dataset, unlabeled_dataset))\n",
        "      for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Iterate over the combined dataset\n",
        "        for step, ((sup_images, labels), unsup_images) in enumerate(combined_dataset):\n",
        "            # Perform a single training step\n",
        "            losses = self.train_step(sup_images, labels, unsup_images)\n",
        "\n",
        "            # Log losses\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: d_loss={losses['d_loss']:.4f}, g_loss={losses['g_loss']:.4f}, c_loss={losses['c_loss']:.4f}\")\n",
        "\n",
        "        # Run callbacks at the end of each epoch\n",
        "        if callbacks:\n",
        "            for callback in callbacks:\n",
        "                callback.on_epoch_end(epoch, logs=losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 7\n",
        "\n",
        "dr = ks.Sequential(name=\"Discriminator\")\n",
        "dr.add(lys.Input(shape=(latent_dim,)))\n",
        "dr.add(lys.Dense(units=180, activation='sigmoid',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dense(units=140, activation='sigmoid',  kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dense(units=120, activation='sigmoid',  kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dropout(0.4))\n",
        "dr.add(lys.Dense(1))\n",
        "dr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "7wkZCKMYhiFD",
        "outputId": "5267d9b6-3ab4-4137-8ca0-edaae7f7fffc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Discriminator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │         \u001b[38;5;34m1,440\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)            │        \u001b[38;5;34m25,340\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m16,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m121\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,440</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,340</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,821\u001b[0m (171.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,821</span> (171.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,821\u001b[0m (171.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,821</span> (171.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr = ks.Sequential(name = \"Generator\")\n",
        "gr.add(lys.Input(shape=(latent_dim,)))\n",
        "gr.add(lys.Dense(180, activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "gr.add(lys.Dense(100, activation='tanh',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "gr.add(lys.Dense(7, activation='relu'))\n",
        "gr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "03XXYLQ1hiL4",
        "outputId": "17dbaeb6-d7dc-4c5d-9b6b-7c02fa0ced10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │         \u001b[38;5;34m1,440\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m18,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m707\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,440</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">707</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,247\u001b[0m (79.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,247</span> (79.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,247\u001b[0m (79.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,247</span> (79.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cr = ks.Sequential(name = \"Classifier\")\n",
        "cr.add(lys.Input(shape = (7,)))\n",
        "cr.add(lys.Dense(256, activation = 'relu'))\n",
        "cr.add(lys.Dense(128, activation = 'relu'))\n",
        "cr.add(lys.Dense(10, activation = 'softmax'))\n",
        "cr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "I6TEDEvqhsOw",
        "outputId": "50afd242-969b-47d8-ab0c-87336bb457f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,234\u001b[0m (141.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,234</span> (141.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,234\u001b[0m (141.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,234</span> (141.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1Z_mEDB60-BnoKCEbiYqxKchk4NSgLcKW\"\n",
        "output_name = \"downloaded_file.xlsx\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_name, quiet=False)\n",
        "data = pd.read_excel(output_name)\n",
        "data = data.to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwLzjaM4jY-P",
        "outputId": "8fe7e6b5-11a0-4197-d8e1-cdd6b64800ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z_mEDB60-BnoKCEbiYqxKchk4NSgLcKW\n",
            "To: /content/downloaded_file.xlsx\n",
            "100%|██████████| 396k/396k [00:00<00:00, 5.67MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "f = input.shape[1]\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(input)\n",
        "input= pca.transform(input)\n",
        "minmax = MinMaxScaler()\n",
        "dataset =  minmax.fit_transform(input).astype(np.float32)\n",
        "#dataset = normalizeData(input).astype(np.float32)  # Convert to float32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(512)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "V5p2MnZmjZrx",
        "outputId": "35f6e86f-eeea-4993-ea52-e1a6d5487ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ninput = data[:, 1:8]\\noutput = data[:, 0]\\noutput = output - 1\\n\\nf = input.shape[1]\\npca = PCA(n_components=7)\\npca.fit(input)\\ninput= pca.transform(input)\\nminmax = MinMaxScaler()\\ndataset =  minmax.fit_transform(input).astype(np.float32)\\n#dataset = normalizeData(input).astype(np.float32)  # Convert to float32\\ndataset = tf.data.Dataset.from_tensor_slices(dataset).batch(512)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.1, random_state=42)\n",
        "\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_labeled_pca = pca.transform(X_labeled)\n",
        "X_unlabeled_pca = pca.transform(X_unlabeled)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "minmax.fit(X_train_pca)\n",
        "\n",
        "X_train_scaled = minmax.transform(X_train_pca).astype(np.float32)\n",
        "X_test_scaled = minmax.transform(X_test_pca).astype(np.float32)\n",
        "X_labeled_scaled = minmax.transform(X_labeled_pca).astype(np.float32)\n",
        "X_unlabeled_scaled = minmax.transform(X_unlabeled_pca).astype(np.float32)\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(y_labeled))\n",
        "y_labeled_onehot = tf.keras.utils.to_categorical(y_labeled, num_classes=num_classes)\n",
        "labels = y_labeled_onehot\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "labeled_dataset = tf.data.Dataset.from_tensor_slices((X_labeled_scaled, y_labeled_onehot))\n",
        "labeled_dataset = labeled_dataset.shuffle(buffer_size=len(X_labeled_scaled)).batch(batch_size)\n",
        "\n",
        "\n",
        "unlabeled_dataset = tf.data.Dataset.from_tensor_slices(X_unlabeled_scaled)\n",
        "unlabeled_dataset = unlabeled_dataset.shuffle(buffer_size=len(X_unlabeled_scaled)).batch(batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_test))\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_onehot))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "sc_sYLCSkrOs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triple_gan = TripleGAN(gr, dr, cr, latent_dim)\n",
        "triple_gan.compile(\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    c_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    g_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        "    d_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        "    c_loss_fn=tf.keras.losses.CategoricalCrossentropy())"
      ],
      "metadata": {
        "id": "qSMx6FnkhiYy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = triple_gan.fit(labeled_dataset, unlabeled_dataset, epochs=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy6jAhnQlFHy",
        "outputId": "5c3dd6e3-3821-4e5e-f6f0-f5c0b5cf6df3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.8296, c_loss=0.2715\n",
            "Epoch 2/2000\n",
            "Step 0: d_loss=1.4010, g_loss=0.6881, c_loss=0.2708\n",
            "Epoch 3/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6280, c_loss=0.2708\n",
            "Epoch 4/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6685, c_loss=0.2696\n",
            "Epoch 5/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.7203, c_loss=0.2691\n",
            "Epoch 6/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.7165, c_loss=0.2684\n",
            "Epoch 7/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6862, c_loss=0.2678\n",
            "Epoch 8/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6809, c_loss=0.2672\n",
            "Epoch 9/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6968, c_loss=0.2666\n",
            "Epoch 10/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.7011, c_loss=0.2660\n",
            "Epoch 11/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6923, c_loss=0.2654\n",
            "Epoch 12/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6903, c_loss=0.2648\n",
            "Epoch 13/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6956, c_loss=0.2642\n",
            "Epoch 14/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6956, c_loss=0.2636\n",
            "Epoch 15/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6930, c_loss=0.2630\n",
            "Epoch 16/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6937, c_loss=0.2624\n",
            "Epoch 17/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6950, c_loss=0.2617\n",
            "Epoch 18/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6941, c_loss=0.2611\n",
            "Epoch 19/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6940, c_loss=0.2605\n",
            "Epoch 20/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6945, c_loss=0.2599\n",
            "Epoch 21/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6942, c_loss=0.2593\n",
            "Epoch 22/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6942, c_loss=0.2588\n",
            "Epoch 23/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6947, c_loss=0.2582\n",
            "Epoch 24/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6943, c_loss=0.2576\n",
            "Epoch 25/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6946, c_loss=0.2570\n",
            "Epoch 26/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6948, c_loss=0.2563\n",
            "Epoch 27/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6943, c_loss=0.2557\n",
            "Epoch 28/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6946, c_loss=0.2551\n",
            "Epoch 29/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6947, c_loss=0.2545\n",
            "Epoch 30/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6944, c_loss=0.2539\n",
            "Epoch 31/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6952, c_loss=0.2534\n",
            "Epoch 32/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6943, c_loss=0.2528\n",
            "Epoch 33/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6949, c_loss=0.2522\n",
            "Epoch 34/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6946, c_loss=0.2517\n",
            "Epoch 35/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6949, c_loss=0.2511\n",
            "Epoch 36/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6940, c_loss=0.2505\n",
            "Epoch 37/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6949, c_loss=0.2499\n",
            "Epoch 38/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6943, c_loss=0.2493\n",
            "Epoch 39/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6946, c_loss=0.2487\n",
            "Epoch 40/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6942, c_loss=0.2481\n",
            "Epoch 41/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6945, c_loss=0.2476\n",
            "Epoch 42/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6950, c_loss=0.2470\n",
            "Epoch 43/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6936, c_loss=0.2465\n",
            "Epoch 44/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6952, c_loss=0.2459\n",
            "Epoch 45/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6930, c_loss=0.2454\n",
            "Epoch 46/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6963, c_loss=0.2449\n",
            "Epoch 47/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6916, c_loss=0.2444\n",
            "Epoch 48/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6978, c_loss=0.2438\n",
            "Epoch 49/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6887, c_loss=0.2432\n",
            "Epoch 50/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.7027, c_loss=0.2426\n",
            "Epoch 51/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6805, c_loss=0.2420\n",
            "Epoch 52/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7155, c_loss=0.2415\n",
            "Epoch 53/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6612, c_loss=0.2409\n",
            "Epoch 54/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.7463, c_loss=0.2404\n",
            "Epoch 55/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6248, c_loss=0.2398\n",
            "Epoch 56/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.7818, c_loss=0.2393\n",
            "Epoch 57/2000\n",
            "Step 0: d_loss=1.3925, g_loss=0.6228, c_loss=0.2388\n",
            "Epoch 58/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.7350, c_loss=0.2383\n",
            "Epoch 59/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6884, c_loss=0.2378\n",
            "Epoch 60/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6791, c_loss=0.2374\n",
            "Epoch 61/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.7121, c_loss=0.2370\n",
            "Epoch 62/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6813, c_loss=0.2365\n",
            "Epoch 63/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6978, c_loss=0.2359\n",
            "Epoch 64/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6949, c_loss=0.2352\n",
            "Epoch 65/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6897, c_loss=0.2346\n",
            "Epoch 66/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6976, c_loss=0.2341\n",
            "Epoch 67/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6912, c_loss=0.2336\n",
            "Epoch 68/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6947, c_loss=0.2331\n",
            "Epoch 69/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6942, c_loss=0.2326\n",
            "Epoch 70/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6932, c_loss=0.2321\n",
            "Epoch 71/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6951, c_loss=0.2316\n",
            "Epoch 72/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6935, c_loss=0.2312\n",
            "Epoch 73/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6948, c_loss=0.2307\n",
            "Epoch 74/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6948, c_loss=0.2302\n",
            "Epoch 75/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6942, c_loss=0.2296\n",
            "Epoch 76/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6952, c_loss=0.2291\n",
            "Epoch 77/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6940, c_loss=0.2285\n",
            "Epoch 78/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6951, c_loss=0.2280\n",
            "Epoch 79/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6949, c_loss=0.2274\n",
            "Epoch 80/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6948, c_loss=0.2269\n",
            "Epoch 81/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6945, c_loss=0.2264\n",
            "Epoch 82/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6951, c_loss=0.2259\n",
            "Epoch 83/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6940, c_loss=0.2254\n",
            "Epoch 84/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6953, c_loss=0.2249\n",
            "Epoch 85/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6937, c_loss=0.2245\n",
            "Epoch 86/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6956, c_loss=0.2240\n",
            "Epoch 87/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6924, c_loss=0.2236\n",
            "Epoch 88/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6955, c_loss=0.2231\n",
            "Epoch 89/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6924, c_loss=0.2227\n",
            "Epoch 90/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6953, c_loss=0.2221\n",
            "Epoch 91/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6922, c_loss=0.2216\n",
            "Epoch 92/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6952, c_loss=0.2211\n",
            "Epoch 93/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6913, c_loss=0.2206\n",
            "Epoch 94/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6950, c_loss=0.2202\n",
            "Epoch 95/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6906, c_loss=0.2197\n",
            "Epoch 96/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6963, c_loss=0.2192\n",
            "Epoch 97/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6882, c_loss=0.2188\n",
            "Epoch 98/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6990, c_loss=0.2183\n",
            "Epoch 99/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6842, c_loss=0.2179\n",
            "Epoch 100/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.7043, c_loss=0.2174\n",
            "Epoch 101/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6766, c_loss=0.2170\n",
            "Epoch 102/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.7164, c_loss=0.2166\n",
            "Epoch 103/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6595, c_loss=0.2162\n",
            "Epoch 104/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.7421, c_loss=0.2158\n",
            "Epoch 105/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6290, c_loss=0.2153\n",
            "Epoch 106/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.7801, c_loss=0.2148\n",
            "Epoch 107/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6062, c_loss=0.2143\n",
            "Epoch 108/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.7747, c_loss=0.2139\n",
            "Epoch 109/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6468, c_loss=0.2134\n",
            "Epoch 110/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.7088, c_loss=0.2129\n",
            "Epoch 111/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.7013, c_loss=0.2125\n",
            "Epoch 112/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6783, c_loss=0.2121\n",
            "Epoch 113/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.7085, c_loss=0.2116\n",
            "Epoch 114/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6863, c_loss=0.2112\n",
            "Epoch 115/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6966, c_loss=0.2108\n",
            "Epoch 116/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6962, c_loss=0.2104\n",
            "Epoch 117/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6916, c_loss=0.2100\n",
            "Epoch 118/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6974, c_loss=0.2096\n",
            "Epoch 119/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6930, c_loss=0.2092\n",
            "Epoch 120/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6953, c_loss=0.2089\n",
            "Epoch 121/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6944, c_loss=0.2086\n",
            "Epoch 122/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6940, c_loss=0.2083\n",
            "Epoch 123/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6953, c_loss=0.2081\n",
            "Epoch 124/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6932, c_loss=0.2077\n",
            "Epoch 125/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6947, c_loss=0.2071\n",
            "Epoch 126/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6930, c_loss=0.2065\n",
            "Epoch 127/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6944, c_loss=0.2059\n",
            "Epoch 128/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6927, c_loss=0.2054\n",
            "Epoch 129/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6943, c_loss=0.2050\n",
            "Epoch 130/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6920, c_loss=0.2046\n",
            "Epoch 131/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6937, c_loss=0.2042\n",
            "Epoch 132/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6929, c_loss=0.2038\n",
            "Epoch 133/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6924, c_loss=0.2034\n",
            "Epoch 134/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6937, c_loss=0.2030\n",
            "Epoch 135/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6914, c_loss=0.2026\n",
            "Epoch 136/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6941, c_loss=0.2022\n",
            "Epoch 137/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6915, c_loss=0.2018\n",
            "Epoch 138/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6936, c_loss=0.2014\n",
            "Epoch 139/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6922, c_loss=0.2011\n",
            "Epoch 140/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6931, c_loss=0.2007\n",
            "Epoch 141/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6928, c_loss=0.2004\n",
            "Epoch 142/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6931, c_loss=0.2000\n",
            "Epoch 143/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6929, c_loss=0.1996\n",
            "Epoch 144/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6931, c_loss=0.1992\n",
            "Epoch 145/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6936, c_loss=0.1987\n",
            "Epoch 146/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6935, c_loss=0.1983\n",
            "Epoch 147/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6940, c_loss=0.1979\n",
            "Epoch 148/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6940, c_loss=0.1976\n",
            "Epoch 149/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6942, c_loss=0.1972\n",
            "Epoch 150/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6938, c_loss=0.1968\n",
            "Epoch 151/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6950, c_loss=0.1964\n",
            "Epoch 152/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6941, c_loss=0.1961\n",
            "Epoch 153/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6950, c_loss=0.1957\n",
            "Epoch 154/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6945, c_loss=0.1954\n",
            "Epoch 155/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6949, c_loss=0.1951\n",
            "Epoch 156/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6940, c_loss=0.1947\n",
            "Epoch 157/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6944, c_loss=0.1943\n",
            "Epoch 158/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6930, c_loss=0.1940\n",
            "Epoch 159/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6962, c_loss=0.1936\n",
            "Epoch 160/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6911, c_loss=0.1932\n",
            "Epoch 161/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6967, c_loss=0.1928\n",
            "Epoch 162/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6903, c_loss=0.1924\n",
            "Epoch 163/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6981, c_loss=0.1920\n",
            "Epoch 164/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6873, c_loss=0.1917\n",
            "Epoch 165/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.7026, c_loss=0.1913\n",
            "Epoch 166/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6804, c_loss=0.1910\n",
            "Epoch 167/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.7117, c_loss=0.1906\n",
            "Epoch 168/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6686, c_loss=0.1903\n",
            "Epoch 169/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.7277, c_loss=0.1900\n",
            "Epoch 170/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6489, c_loss=0.1896\n",
            "Epoch 171/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.7541, c_loss=0.1892\n",
            "Epoch 172/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6215, c_loss=0.1889\n",
            "Epoch 173/2000\n",
            "Step 0: d_loss=1.3922, g_loss=0.7835, c_loss=0.1885\n",
            "Epoch 174/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.6077, c_loss=0.1881\n",
            "Epoch 175/2000\n",
            "Step 0: d_loss=1.3949, g_loss=0.7733, c_loss=0.1878\n",
            "Epoch 176/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6410, c_loss=0.1874\n",
            "Epoch 177/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.7193, c_loss=0.1871\n",
            "Epoch 178/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6883, c_loss=0.1868\n",
            "Epoch 179/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6857, c_loss=0.1864\n",
            "Epoch 180/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.7035, c_loss=0.1862\n",
            "Epoch 181/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6834, c_loss=0.1858\n",
            "Epoch 182/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6985, c_loss=0.1855\n",
            "Epoch 183/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6904, c_loss=0.1851\n",
            "Epoch 184/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6927, c_loss=0.1848\n",
            "Epoch 185/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6942, c_loss=0.1844\n",
            "Epoch 186/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6915, c_loss=0.1841\n",
            "Epoch 187/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6941, c_loss=0.1837\n",
            "Epoch 188/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6928, c_loss=0.1834\n",
            "Epoch 189/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6938, c_loss=0.1831\n",
            "Epoch 190/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6937, c_loss=0.1828\n",
            "Epoch 191/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6936, c_loss=0.1824\n",
            "Epoch 192/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6945, c_loss=0.1821\n",
            "Epoch 193/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6938, c_loss=0.1818\n",
            "Epoch 194/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6947, c_loss=0.1814\n",
            "Epoch 195/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6942, c_loss=0.1811\n",
            "Epoch 196/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6944, c_loss=0.1807\n",
            "Epoch 197/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6950, c_loss=0.1804\n",
            "Epoch 198/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6941, c_loss=0.1800\n",
            "Epoch 199/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6947, c_loss=0.1797\n",
            "Epoch 200/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6942, c_loss=0.1794\n",
            "Epoch 201/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6942, c_loss=0.1790\n",
            "Epoch 202/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6943, c_loss=0.1787\n",
            "Epoch 203/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6936, c_loss=0.1784\n",
            "Epoch 204/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6944, c_loss=0.1781\n",
            "Epoch 205/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6939, c_loss=0.1778\n",
            "Epoch 206/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6928, c_loss=0.1775\n",
            "Epoch 207/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6950, c_loss=0.1772\n",
            "Epoch 208/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6919, c_loss=0.1769\n",
            "Epoch 209/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6954, c_loss=0.1766\n",
            "Epoch 210/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6912, c_loss=0.1763\n",
            "Epoch 211/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6950, c_loss=0.1760\n",
            "Epoch 212/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6914, c_loss=0.1757\n",
            "Epoch 213/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6943, c_loss=0.1753\n",
            "Epoch 214/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6921, c_loss=0.1750\n",
            "Epoch 215/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6939, c_loss=0.1747\n",
            "Epoch 216/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6913, c_loss=0.1743\n",
            "Epoch 217/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6941, c_loss=0.1740\n",
            "Epoch 218/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6911, c_loss=0.1737\n",
            "Epoch 219/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6946, c_loss=0.1734\n",
            "Epoch 220/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6902, c_loss=0.1731\n",
            "Epoch 221/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6950, c_loss=0.1728\n",
            "Epoch 222/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6896, c_loss=0.1725\n",
            "Epoch 223/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6959, c_loss=0.1722\n",
            "Epoch 224/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6894, c_loss=0.1719\n",
            "Epoch 225/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6953, c_loss=0.1716\n",
            "Epoch 226/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6905, c_loss=0.1713\n",
            "Epoch 227/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6945, c_loss=0.1710\n",
            "Epoch 228/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6911, c_loss=0.1708\n",
            "Epoch 229/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6946, c_loss=0.1705\n",
            "Epoch 230/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6916, c_loss=0.1702\n",
            "Epoch 231/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6944, c_loss=0.1699\n",
            "Epoch 232/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6926, c_loss=0.1696\n",
            "Epoch 233/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6945, c_loss=0.1693\n",
            "Epoch 234/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6926, c_loss=0.1690\n",
            "Epoch 235/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6948, c_loss=0.1687\n",
            "Epoch 236/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6929, c_loss=0.1684\n",
            "Epoch 237/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6955, c_loss=0.1681\n",
            "Epoch 238/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6931, c_loss=0.1678\n",
            "Epoch 239/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6955, c_loss=0.1675\n",
            "Epoch 240/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6938, c_loss=0.1672\n",
            "Epoch 241/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6948, c_loss=0.1669\n",
            "Epoch 242/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6947, c_loss=0.1666\n",
            "Epoch 243/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6942, c_loss=0.1664\n",
            "Epoch 244/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6954, c_loss=0.1661\n",
            "Epoch 245/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6937, c_loss=0.1659\n",
            "Epoch 246/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6961, c_loss=0.1657\n",
            "Epoch 247/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6926, c_loss=0.1655\n",
            "Epoch 248/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6985, c_loss=0.1653\n",
            "Epoch 249/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6888, c_loss=0.1650\n",
            "Epoch 250/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.7019, c_loss=0.1647\n",
            "Epoch 251/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6844, c_loss=0.1643\n",
            "Epoch 252/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.7061, c_loss=0.1639\n",
            "Epoch 253/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6777, c_loss=0.1636\n",
            "Epoch 254/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.7154, c_loss=0.1633\n",
            "Epoch 255/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6641, c_loss=0.1630\n",
            "Epoch 256/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.7345, c_loss=0.1627\n",
            "Epoch 257/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6405, c_loss=0.1624\n",
            "Epoch 258/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.7669, c_loss=0.1621\n",
            "Epoch 259/2000\n",
            "Step 0: d_loss=1.3935, g_loss=0.6055, c_loss=0.1619\n",
            "Epoch 260/2000\n",
            "Step 0: d_loss=1.3966, g_loss=0.8045, c_loss=0.1616\n",
            "Epoch 261/2000\n",
            "Step 0: d_loss=1.4001, g_loss=0.5888, c_loss=0.1614\n",
            "Epoch 262/2000\n",
            "Step 0: d_loss=1.4003, g_loss=0.7891, c_loss=0.1611\n",
            "Epoch 263/2000\n",
            "Step 0: d_loss=1.3974, g_loss=0.6335, c_loss=0.1609\n",
            "Epoch 264/2000\n",
            "Step 0: d_loss=1.3928, g_loss=0.7178, c_loss=0.1607\n",
            "Epoch 265/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6926, c_loss=0.1605\n",
            "Epoch 266/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6786, c_loss=0.1602\n",
            "Epoch 267/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.7074, c_loss=0.1599\n",
            "Epoch 268/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6808, c_loss=0.1596\n",
            "Epoch 269/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6975, c_loss=0.1593\n",
            "Epoch 270/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6913, c_loss=0.1590\n",
            "Epoch 271/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6906, c_loss=0.1587\n",
            "Epoch 272/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6949, c_loss=0.1584\n",
            "Epoch 273/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6904, c_loss=0.1582\n",
            "Epoch 274/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6941, c_loss=0.1579\n",
            "Epoch 275/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6923, c_loss=0.1576\n",
            "Epoch 276/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6932, c_loss=0.1574\n",
            "Epoch 277/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6938, c_loss=0.1571\n",
            "Epoch 278/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6933, c_loss=0.1569\n",
            "Epoch 279/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6944, c_loss=0.1567\n",
            "Epoch 280/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6938, c_loss=0.1564\n",
            "Epoch 281/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6945, c_loss=0.1562\n",
            "Epoch 282/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6947, c_loss=0.1560\n",
            "Epoch 283/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6946, c_loss=0.1557\n",
            "Epoch 284/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6949, c_loss=0.1555\n",
            "Epoch 285/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6949, c_loss=0.1552\n",
            "Epoch 286/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6949, c_loss=0.1550\n",
            "Epoch 287/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6952, c_loss=0.1547\n",
            "Epoch 288/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6948, c_loss=0.1544\n",
            "Epoch 289/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6956, c_loss=0.1542\n",
            "Epoch 290/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6948, c_loss=0.1539\n",
            "Epoch 291/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6947, c_loss=0.1536\n",
            "Epoch 292/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6948, c_loss=0.1534\n",
            "Epoch 293/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6945, c_loss=0.1531\n",
            "Epoch 294/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6944, c_loss=0.1529\n",
            "Epoch 295/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6948, c_loss=0.1526\n",
            "Epoch 296/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6929, c_loss=0.1524\n",
            "Epoch 297/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6946, c_loss=0.1522\n",
            "Epoch 298/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6927, c_loss=0.1519\n",
            "Epoch 299/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6935, c_loss=0.1517\n",
            "Epoch 300/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6933, c_loss=0.1515\n",
            "Epoch 301/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6933, c_loss=0.1513\n",
            "Epoch 302/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6930, c_loss=0.1511\n",
            "Epoch 303/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6931, c_loss=0.1508\n",
            "Epoch 304/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6924, c_loss=0.1506\n",
            "Epoch 305/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6928, c_loss=0.1503\n",
            "Epoch 306/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6917, c_loss=0.1501\n",
            "Epoch 307/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6934, c_loss=0.1498\n",
            "Epoch 308/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6913, c_loss=0.1496\n",
            "Epoch 309/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6935, c_loss=0.1493\n",
            "Epoch 310/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6921, c_loss=0.1491\n",
            "Epoch 311/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6932, c_loss=0.1488\n",
            "Epoch 312/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6927, c_loss=0.1486\n",
            "Epoch 313/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6923, c_loss=0.1484\n",
            "Epoch 314/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6930, c_loss=0.1482\n",
            "Epoch 315/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6931, c_loss=0.1480\n",
            "Epoch 316/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6927, c_loss=0.1478\n",
            "Epoch 317/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6938, c_loss=0.1475\n",
            "Epoch 318/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6926, c_loss=0.1473\n",
            "Epoch 319/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6944, c_loss=0.1470\n",
            "Epoch 320/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6920, c_loss=0.1468\n",
            "Epoch 321/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6948, c_loss=0.1465\n",
            "Epoch 322/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6924, c_loss=0.1463\n",
            "Epoch 323/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6954, c_loss=0.1460\n",
            "Epoch 324/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6921, c_loss=0.1458\n",
            "Epoch 325/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6958, c_loss=0.1455\n",
            "Epoch 326/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6922, c_loss=0.1453\n",
            "Epoch 327/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6964, c_loss=0.1451\n",
            "Epoch 328/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6917, c_loss=0.1449\n",
            "Epoch 329/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6969, c_loss=0.1447\n",
            "Epoch 330/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6924, c_loss=0.1445\n",
            "Epoch 331/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6967, c_loss=0.1443\n",
            "Epoch 332/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6925, c_loss=0.1440\n",
            "Epoch 333/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6969, c_loss=0.1438\n",
            "Epoch 334/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6929, c_loss=0.1436\n",
            "Epoch 335/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6971, c_loss=0.1434\n",
            "Epoch 336/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6930, c_loss=0.1432\n",
            "Epoch 337/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6962, c_loss=0.1430\n",
            "Epoch 338/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6935, c_loss=0.1427\n",
            "Epoch 339/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6952, c_loss=0.1425\n",
            "Epoch 340/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6949, c_loss=0.1423\n",
            "Epoch 341/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6940, c_loss=0.1421\n",
            "Epoch 342/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6956, c_loss=0.1419\n",
            "Epoch 343/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6914, c_loss=0.1417\n",
            "Epoch 344/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6970, c_loss=0.1415\n",
            "Epoch 345/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6899, c_loss=0.1413\n",
            "Epoch 346/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6977, c_loss=0.1411\n",
            "Epoch 347/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6890, c_loss=0.1408\n",
            "Epoch 348/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6984, c_loss=0.1406\n",
            "Epoch 349/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6865, c_loss=0.1404\n",
            "Epoch 350/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.7000, c_loss=0.1401\n",
            "Epoch 351/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6844, c_loss=0.1399\n",
            "Epoch 352/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.7024, c_loss=0.1397\n",
            "Epoch 353/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6811, c_loss=0.1395\n",
            "Epoch 354/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.7059, c_loss=0.1392\n",
            "Epoch 355/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6765, c_loss=0.1390\n",
            "Epoch 356/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.7113, c_loss=0.1388\n",
            "Epoch 357/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6698, c_loss=0.1386\n",
            "Epoch 358/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.7187, c_loss=0.1384\n",
            "Epoch 359/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6620, c_loss=0.1382\n",
            "Epoch 360/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.7285, c_loss=0.1380\n",
            "Epoch 361/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6510, c_loss=0.1378\n",
            "Epoch 362/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.7413, c_loss=0.1375\n",
            "Epoch 363/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6400, c_loss=0.1373\n",
            "Epoch 364/2000\n",
            "Step 0: d_loss=1.3924, g_loss=0.7520, c_loss=0.1371\n",
            "Epoch 365/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.6329, c_loss=0.1369\n",
            "Epoch 366/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.7560, c_loss=0.1367\n",
            "Epoch 367/2000\n",
            "Step 0: d_loss=1.3925, g_loss=0.6357, c_loss=0.1365\n",
            "Epoch 368/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.7462, c_loss=0.1363\n",
            "Epoch 369/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6505, c_loss=0.1361\n",
            "Epoch 370/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.7275, c_loss=0.1359\n",
            "Epoch 371/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6699, c_loss=0.1357\n",
            "Epoch 372/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.7097, c_loss=0.1355\n",
            "Epoch 373/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6848, c_loss=0.1353\n",
            "Epoch 374/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6993, c_loss=0.1352\n",
            "Epoch 375/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6929, c_loss=0.1350\n",
            "Epoch 376/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6953, c_loss=0.1348\n",
            "Epoch 377/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6958, c_loss=0.1346\n",
            "Epoch 378/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6947, c_loss=0.1345\n",
            "Epoch 379/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6966, c_loss=0.1343\n",
            "Epoch 380/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6949, c_loss=0.1342\n",
            "Epoch 381/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6970, c_loss=0.1340\n",
            "Epoch 382/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6953, c_loss=0.1338\n",
            "Epoch 383/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6961, c_loss=0.1336\n",
            "Epoch 384/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6960, c_loss=0.1334\n",
            "Epoch 385/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6956, c_loss=0.1331\n",
            "Epoch 386/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6962, c_loss=0.1328\n",
            "Epoch 387/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6947, c_loss=0.1325\n",
            "Epoch 388/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6954, c_loss=0.1323\n",
            "Epoch 389/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6939, c_loss=0.1320\n",
            "Epoch 390/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6956, c_loss=0.1318\n",
            "Epoch 391/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6925, c_loss=0.1316\n",
            "Epoch 392/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6959, c_loss=0.1314\n",
            "Epoch 393/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6907, c_loss=0.1312\n",
            "Epoch 394/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6956, c_loss=0.1310\n",
            "Epoch 395/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6900, c_loss=0.1308\n",
            "Epoch 396/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6958, c_loss=0.1306\n",
            "Epoch 397/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6882, c_loss=0.1304\n",
            "Epoch 398/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6959, c_loss=0.1302\n",
            "Epoch 399/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6876, c_loss=0.1300\n",
            "Epoch 400/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6957, c_loss=0.1298\n",
            "Epoch 401/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6874, c_loss=0.1297\n",
            "Epoch 402/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6952, c_loss=0.1295\n",
            "Epoch 403/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6877, c_loss=0.1293\n",
            "Epoch 404/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6954, c_loss=0.1291\n",
            "Epoch 405/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6879, c_loss=0.1289\n",
            "Epoch 406/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6953, c_loss=0.1287\n",
            "Epoch 407/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6881, c_loss=0.1286\n",
            "Epoch 408/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6958, c_loss=0.1284\n",
            "Epoch 409/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6882, c_loss=0.1282\n",
            "Epoch 410/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6963, c_loss=0.1281\n",
            "Epoch 411/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6884, c_loss=0.1279\n",
            "Epoch 412/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6969, c_loss=0.1277\n",
            "Epoch 413/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6884, c_loss=0.1276\n",
            "Epoch 414/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6980, c_loss=0.1274\n",
            "Epoch 415/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6880, c_loss=0.1273\n",
            "Epoch 416/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6990, c_loss=0.1271\n",
            "Epoch 417/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6880, c_loss=0.1270\n",
            "Epoch 418/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6995, c_loss=0.1268\n",
            "Epoch 419/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6876, c_loss=0.1267\n",
            "Epoch 420/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.7005, c_loss=0.1265\n",
            "Epoch 421/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6866, c_loss=0.1263\n",
            "Epoch 422/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.7018, c_loss=0.1261\n",
            "Epoch 423/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6857, c_loss=0.1258\n",
            "Epoch 424/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.7032, c_loss=0.1256\n",
            "Epoch 425/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6841, c_loss=0.1254\n",
            "Epoch 426/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.7052, c_loss=0.1252\n",
            "Epoch 427/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6821, c_loss=0.1250\n",
            "Epoch 428/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.7079, c_loss=0.1248\n",
            "Epoch 429/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6791, c_loss=0.1246\n",
            "Epoch 430/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.7113, c_loss=0.1244\n",
            "Epoch 431/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6753, c_loss=0.1242\n",
            "Epoch 432/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.7162, c_loss=0.1240\n",
            "Epoch 433/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6693, c_loss=0.1238\n",
            "Epoch 434/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.7237, c_loss=0.1237\n",
            "Epoch 435/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6622, c_loss=0.1235\n",
            "Epoch 436/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.7323, c_loss=0.1233\n",
            "Epoch 437/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6537, c_loss=0.1232\n",
            "Epoch 438/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7422, c_loss=0.1230\n",
            "Epoch 439/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6449, c_loss=0.1228\n",
            "Epoch 440/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.7511, c_loss=0.1227\n",
            "Epoch 441/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6388, c_loss=0.1225\n",
            "Epoch 442/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.7544, c_loss=0.1223\n",
            "Epoch 443/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6394, c_loss=0.1222\n",
            "Epoch 444/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.7491, c_loss=0.1220\n",
            "Epoch 445/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6481, c_loss=0.1218\n",
            "Epoch 446/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.7349, c_loss=0.1216\n",
            "Epoch 447/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6627, c_loss=0.1214\n",
            "Epoch 448/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.7186, c_loss=0.1212\n",
            "Epoch 449/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6760, c_loss=0.1211\n",
            "Epoch 450/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7063, c_loss=0.1209\n",
            "Epoch 451/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6846, c_loss=0.1207\n",
            "Epoch 452/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6992, c_loss=0.1205\n",
            "Epoch 453/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6889, c_loss=0.1203\n",
            "Epoch 454/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6956, c_loss=0.1201\n",
            "Epoch 455/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6905, c_loss=0.1200\n",
            "Epoch 456/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6935, c_loss=0.1198\n",
            "Epoch 457/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6914, c_loss=0.1196\n",
            "Epoch 458/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6924, c_loss=0.1195\n",
            "Epoch 459/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6921, c_loss=0.1193\n",
            "Epoch 460/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6910, c_loss=0.1191\n",
            "Epoch 461/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6927, c_loss=0.1190\n",
            "Epoch 462/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6903, c_loss=0.1188\n",
            "Epoch 463/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6923, c_loss=0.1187\n",
            "Epoch 464/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6909, c_loss=0.1185\n",
            "Epoch 465/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6919, c_loss=0.1184\n",
            "Epoch 466/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6909, c_loss=0.1182\n",
            "Epoch 467/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6920, c_loss=0.1180\n",
            "Epoch 468/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6912, c_loss=0.1178\n",
            "Epoch 469/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6920, c_loss=0.1177\n",
            "Epoch 470/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6914, c_loss=0.1175\n",
            "Epoch 471/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6920, c_loss=0.1173\n",
            "Epoch 472/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6918, c_loss=0.1172\n",
            "Epoch 473/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6924, c_loss=0.1170\n",
            "Epoch 474/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6921, c_loss=0.1168\n",
            "Epoch 475/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6925, c_loss=0.1167\n",
            "Epoch 476/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6927, c_loss=0.1165\n",
            "Epoch 477/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6931, c_loss=0.1164\n",
            "Epoch 478/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6929, c_loss=0.1162\n",
            "Epoch 479/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6935, c_loss=0.1161\n",
            "Epoch 480/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6936, c_loss=0.1159\n",
            "Epoch 481/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6935, c_loss=0.1158\n",
            "Epoch 482/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6948, c_loss=0.1157\n",
            "Epoch 483/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6937, c_loss=0.1155\n",
            "Epoch 484/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6951, c_loss=0.1154\n",
            "Epoch 485/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6949, c_loss=0.1152\n",
            "Epoch 486/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6955, c_loss=0.1150\n",
            "Epoch 487/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6954, c_loss=0.1149\n",
            "Epoch 488/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6957, c_loss=0.1147\n",
            "Epoch 489/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6961, c_loss=0.1146\n",
            "Epoch 490/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6958, c_loss=0.1144\n",
            "Epoch 491/2000\n",
            "Step 0: d_loss=1.3809, g_loss=0.6962, c_loss=0.1142\n",
            "Epoch 492/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6968, c_loss=0.1141\n",
            "Epoch 493/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6962, c_loss=0.1140\n",
            "Epoch 494/2000\n",
            "Step 0: d_loss=1.3806, g_loss=0.6973, c_loss=0.1138\n",
            "Epoch 495/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6957, c_loss=0.1137\n",
            "Epoch 496/2000\n",
            "Step 0: d_loss=1.3809, g_loss=0.6975, c_loss=0.1135\n",
            "Epoch 497/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6954, c_loss=0.1134\n",
            "Epoch 498/2000\n",
            "Step 0: d_loss=1.3813, g_loss=0.6976, c_loss=0.1132\n",
            "Epoch 499/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6945, c_loss=0.1131\n",
            "Epoch 500/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6975, c_loss=0.1129\n",
            "Epoch 501/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6937, c_loss=0.1127\n",
            "Epoch 502/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6974, c_loss=0.1125\n",
            "Epoch 503/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6920, c_loss=0.1123\n",
            "Epoch 504/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6978, c_loss=0.1122\n",
            "Epoch 505/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6895, c_loss=0.1120\n",
            "Epoch 506/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6984, c_loss=0.1118\n",
            "Epoch 507/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6874, c_loss=0.1117\n",
            "Epoch 508/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6991, c_loss=0.1115\n",
            "Epoch 509/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6847, c_loss=0.1114\n",
            "Epoch 510/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.7004, c_loss=0.1112\n",
            "Epoch 511/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6805, c_loss=0.1111\n",
            "Epoch 512/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.7043, c_loss=0.1109\n",
            "Epoch 513/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.6744, c_loss=0.1108\n",
            "Epoch 514/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7108, c_loss=0.1107\n",
            "Epoch 515/2000\n",
            "Step 0: d_loss=1.3932, g_loss=0.6665, c_loss=0.1106\n",
            "Epoch 516/2000\n",
            "Step 0: d_loss=1.3938, g_loss=0.7197, c_loss=0.1104\n",
            "Epoch 517/2000\n",
            "Step 0: d_loss=1.3945, g_loss=0.6560, c_loss=0.1103\n",
            "Epoch 518/2000\n",
            "Step 0: d_loss=1.3953, g_loss=0.7330, c_loss=0.1102\n",
            "Epoch 519/2000\n",
            "Step 0: d_loss=1.3949, g_loss=0.6405, c_loss=0.1101\n",
            "Epoch 520/2000\n",
            "Step 0: d_loss=1.3960, g_loss=0.7521, c_loss=0.1099\n",
            "Epoch 521/2000\n",
            "Step 0: d_loss=1.3967, g_loss=0.6244, c_loss=0.1098\n",
            "Epoch 522/2000\n",
            "Step 0: d_loss=1.3976, g_loss=0.7682, c_loss=0.1097\n",
            "Epoch 523/2000\n",
            "Step 0: d_loss=1.3977, g_loss=0.6156, c_loss=0.1095\n",
            "Epoch 524/2000\n",
            "Step 0: d_loss=1.3983, g_loss=0.7709, c_loss=0.1093\n",
            "Epoch 525/2000\n",
            "Step 0: d_loss=1.3975, g_loss=0.6241, c_loss=0.1092\n",
            "Epoch 526/2000\n",
            "Step 0: d_loss=1.3959, g_loss=0.7506, c_loss=0.1090\n",
            "Epoch 527/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.6503, c_loss=0.1089\n",
            "Epoch 528/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.7208, c_loss=0.1087\n",
            "Epoch 529/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6771, c_loss=0.1085\n",
            "Epoch 530/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6997, c_loss=0.1084\n",
            "Epoch 531/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6926, c_loss=0.1082\n",
            "Epoch 532/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6917, c_loss=0.1081\n",
            "Epoch 533/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6973, c_loss=0.1079\n",
            "Epoch 534/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6912, c_loss=0.1078\n",
            "Epoch 535/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6975, c_loss=0.1076\n",
            "Epoch 536/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6931, c_loss=0.1075\n",
            "Epoch 537/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6966, c_loss=0.1073\n",
            "Epoch 538/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6950, c_loss=0.1072\n",
            "Epoch 539/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6959, c_loss=0.1071\n",
            "Epoch 540/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6960, c_loss=0.1069\n",
            "Epoch 541/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6960, c_loss=0.1068\n",
            "Epoch 542/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6962, c_loss=0.1067\n",
            "Epoch 543/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6963, c_loss=0.1066\n",
            "Epoch 544/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6963, c_loss=0.1065\n",
            "Epoch 545/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6965, c_loss=0.1064\n",
            "Epoch 546/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6967, c_loss=0.1062\n",
            "Epoch 547/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6963, c_loss=0.1061\n",
            "Epoch 548/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6970, c_loss=0.1059\n",
            "Epoch 549/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6953, c_loss=0.1058\n",
            "Epoch 550/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6975, c_loss=0.1056\n",
            "Epoch 551/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6954, c_loss=0.1054\n",
            "Epoch 552/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6968, c_loss=0.1053\n",
            "Epoch 553/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6958, c_loss=0.1051\n",
            "Epoch 554/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6958, c_loss=0.1050\n",
            "Epoch 555/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6960, c_loss=0.1048\n",
            "Epoch 556/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6955, c_loss=0.1047\n",
            "Epoch 557/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6950, c_loss=0.1046\n",
            "Epoch 558/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6951, c_loss=0.1044\n",
            "Epoch 559/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6944, c_loss=0.1043\n",
            "Epoch 560/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6954, c_loss=0.1042\n",
            "Epoch 561/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6932, c_loss=0.1041\n",
            "Epoch 562/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6950, c_loss=0.1040\n",
            "Epoch 563/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6929, c_loss=0.1038\n",
            "Epoch 564/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6940, c_loss=0.1037\n",
            "Epoch 565/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6929, c_loss=0.1036\n",
            "Epoch 566/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6936, c_loss=0.1035\n",
            "Epoch 567/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6923, c_loss=0.1034\n",
            "Epoch 568/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6923, c_loss=0.1032\n",
            "Epoch 569/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6927, c_loss=0.1031\n",
            "Epoch 570/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6915, c_loss=0.1030\n",
            "Epoch 571/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6926, c_loss=0.1029\n",
            "Epoch 572/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6908, c_loss=0.1027\n",
            "Epoch 573/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6918, c_loss=0.1026\n",
            "Epoch 574/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6909, c_loss=0.1024\n",
            "Epoch 575/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6910, c_loss=0.1023\n",
            "Epoch 576/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6911, c_loss=0.1021\n",
            "Epoch 577/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6895, c_loss=0.1020\n",
            "Epoch 578/2000\n",
            "Step 0: d_loss=1.3918, g_loss=0.6924, c_loss=0.1019\n",
            "Epoch 579/2000\n",
            "Step 0: d_loss=1.3922, g_loss=0.6884, c_loss=0.1017\n",
            "Epoch 580/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6922, c_loss=0.1016\n",
            "Epoch 581/2000\n",
            "Step 0: d_loss=1.3921, g_loss=0.6893, c_loss=0.1015\n",
            "Epoch 582/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6913, c_loss=0.1014\n",
            "Epoch 583/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6899, c_loss=0.1012\n",
            "Epoch 584/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6914, c_loss=0.1011\n",
            "Epoch 585/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6900, c_loss=0.1010\n",
            "Epoch 586/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6911, c_loss=0.1008\n",
            "Epoch 587/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.6903, c_loss=0.1007\n",
            "Epoch 588/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6914, c_loss=0.1006\n",
            "Epoch 589/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6910, c_loss=0.1004\n",
            "Epoch 590/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6919, c_loss=0.1003\n",
            "Epoch 591/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6910, c_loss=0.1002\n",
            "Epoch 592/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6924, c_loss=0.1000\n",
            "Epoch 593/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6913, c_loss=0.0999\n",
            "Epoch 594/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6929, c_loss=0.0998\n",
            "Epoch 595/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6922, c_loss=0.0997\n",
            "Epoch 596/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6931, c_loss=0.0996\n",
            "Epoch 597/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6928, c_loss=0.0995\n",
            "Epoch 598/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6939, c_loss=0.0995\n",
            "Epoch 599/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6933, c_loss=0.0994\n",
            "Epoch 600/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6948, c_loss=0.0993\n",
            "Epoch 601/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6939, c_loss=0.0993\n",
            "Epoch 602/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6958, c_loss=0.0991\n",
            "Epoch 603/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6942, c_loss=0.0989\n",
            "Epoch 604/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6966, c_loss=0.0987\n",
            "Epoch 605/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6950, c_loss=0.0986\n",
            "Epoch 606/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6974, c_loss=0.0984\n",
            "Epoch 607/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6955, c_loss=0.0982\n",
            "Epoch 608/2000\n",
            "Step 0: d_loss=1.3809, g_loss=0.6980, c_loss=0.0981\n",
            "Epoch 609/2000\n",
            "Step 0: d_loss=1.3805, g_loss=0.6958, c_loss=0.0980\n",
            "Epoch 610/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6987, c_loss=0.0978\n",
            "Epoch 611/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6960, c_loss=0.0977\n",
            "Epoch 612/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6989, c_loss=0.0976\n",
            "Epoch 613/2000\n",
            "Step 0: d_loss=1.3789, g_loss=0.6961, c_loss=0.0975\n",
            "Epoch 614/2000\n",
            "Step 0: d_loss=1.3790, g_loss=0.7004, c_loss=0.0973\n",
            "Epoch 615/2000\n",
            "Step 0: d_loss=1.3793, g_loss=0.6950, c_loss=0.0972\n",
            "Epoch 616/2000\n",
            "Step 0: d_loss=1.3788, g_loss=0.7007, c_loss=0.0971\n",
            "Epoch 617/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6952, c_loss=0.0970\n",
            "Epoch 618/2000\n",
            "Step 0: d_loss=1.3796, g_loss=0.6999, c_loss=0.0969\n",
            "Epoch 619/2000\n",
            "Step 0: d_loss=1.3803, g_loss=0.6949, c_loss=0.0968\n",
            "Epoch 620/2000\n",
            "Step 0: d_loss=1.3802, g_loss=0.7000, c_loss=0.0967\n",
            "Epoch 621/2000\n",
            "Step 0: d_loss=1.3810, g_loss=0.6942, c_loss=0.0966\n",
            "Epoch 622/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6990, c_loss=0.0964\n",
            "Epoch 623/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6933, c_loss=0.0963\n",
            "Epoch 624/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.7003, c_loss=0.0962\n",
            "Epoch 625/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6918, c_loss=0.0961\n",
            "Epoch 626/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6993, c_loss=0.0960\n",
            "Epoch 627/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6904, c_loss=0.0959\n",
            "Epoch 628/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.7002, c_loss=0.0957\n",
            "Epoch 629/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6865, c_loss=0.0956\n",
            "Epoch 630/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.7033, c_loss=0.0955\n",
            "Epoch 631/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6812, c_loss=0.0954\n",
            "Epoch 632/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.7087, c_loss=0.0953\n",
            "Epoch 633/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6739, c_loss=0.0952\n",
            "Epoch 634/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.7160, c_loss=0.0951\n",
            "Epoch 635/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6641, c_loss=0.0950\n",
            "Epoch 636/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.7275, c_loss=0.0949\n",
            "Epoch 637/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.6476, c_loss=0.0948\n",
            "Epoch 638/2000\n",
            "Step 0: d_loss=1.3945, g_loss=0.7477, c_loss=0.0947\n",
            "Epoch 639/2000\n",
            "Step 0: d_loss=1.3958, g_loss=0.6253, c_loss=0.0946\n",
            "Epoch 640/2000\n",
            "Step 0: d_loss=1.3980, g_loss=0.7728, c_loss=0.0944\n",
            "Epoch 641/2000\n",
            "Step 0: d_loss=1.4007, g_loss=0.6037, c_loss=0.0943\n",
            "Epoch 642/2000\n",
            "Step 0: d_loss=1.4029, g_loss=0.7902, c_loss=0.0941\n",
            "Epoch 643/2000\n",
            "Step 0: d_loss=1.4041, g_loss=0.5980, c_loss=0.0940\n",
            "Epoch 644/2000\n",
            "Step 0: d_loss=1.4049, g_loss=0.7796, c_loss=0.0939\n",
            "Epoch 645/2000\n",
            "Step 0: d_loss=1.4026, g_loss=0.6216, c_loss=0.0938\n",
            "Epoch 646/2000\n",
            "Step 0: d_loss=1.4000, g_loss=0.7387, c_loss=0.0936\n",
            "Epoch 647/2000\n",
            "Step 0: d_loss=1.3975, g_loss=0.6627, c_loss=0.0935\n",
            "Epoch 648/2000\n",
            "Step 0: d_loss=1.3951, g_loss=0.6991, c_loss=0.0934\n",
            "Epoch 649/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.6908, c_loss=0.0933\n",
            "Epoch 650/2000\n",
            "Step 0: d_loss=1.3942, g_loss=0.6839, c_loss=0.0932\n",
            "Epoch 651/2000\n",
            "Step 0: d_loss=1.3925, g_loss=0.6966, c_loss=0.0931\n",
            "Epoch 652/2000\n",
            "Step 0: d_loss=1.3918, g_loss=0.6859, c_loss=0.0930\n",
            "Epoch 653/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6948, c_loss=0.0929\n",
            "Epoch 654/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6899, c_loss=0.0928\n",
            "Epoch 655/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6934, c_loss=0.0927\n",
            "Epoch 656/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6933, c_loss=0.0926\n",
            "Epoch 657/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6937, c_loss=0.0925\n",
            "Epoch 658/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6954, c_loss=0.0923\n",
            "Epoch 659/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6946, c_loss=0.0922\n",
            "Epoch 660/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6968, c_loss=0.0921\n",
            "Epoch 661/2000\n",
            "Step 0: d_loss=1.3811, g_loss=0.6959, c_loss=0.0920\n",
            "Epoch 662/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.6974, c_loss=0.0919\n",
            "Epoch 663/2000\n",
            "Step 0: d_loss=1.3791, g_loss=0.6972, c_loss=0.0918\n",
            "Epoch 664/2000\n",
            "Step 0: d_loss=1.3788, g_loss=0.6982, c_loss=0.0917\n",
            "Epoch 665/2000\n",
            "Step 0: d_loss=1.3781, g_loss=0.6979, c_loss=0.0915\n",
            "Epoch 666/2000\n",
            "Step 0: d_loss=1.3779, g_loss=0.6985, c_loss=0.0914\n",
            "Epoch 667/2000\n",
            "Step 0: d_loss=1.3780, g_loss=0.6981, c_loss=0.0913\n",
            "Epoch 668/2000\n",
            "Step 0: d_loss=1.3778, g_loss=0.6985, c_loss=0.0912\n",
            "Epoch 669/2000\n",
            "Step 0: d_loss=1.3782, g_loss=0.6981, c_loss=0.0911\n",
            "Epoch 670/2000\n",
            "Step 0: d_loss=1.3782, g_loss=0.6979, c_loss=0.0910\n",
            "Epoch 671/2000\n",
            "Step 0: d_loss=1.3783, g_loss=0.6984, c_loss=0.0909\n",
            "Epoch 672/2000\n",
            "Step 0: d_loss=1.3794, g_loss=0.6982, c_loss=0.0907\n",
            "Epoch 673/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6963, c_loss=0.0906\n",
            "Epoch 674/2000\n",
            "Step 0: d_loss=1.3802, g_loss=0.6977, c_loss=0.0905\n",
            "Epoch 675/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6964, c_loss=0.0904\n",
            "Epoch 676/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6965, c_loss=0.0903\n",
            "Epoch 677/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6964, c_loss=0.0903\n",
            "Epoch 678/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6941, c_loss=0.0902\n",
            "Epoch 679/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6966, c_loss=0.0901\n",
            "Epoch 680/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6939, c_loss=0.0900\n",
            "Epoch 681/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6954, c_loss=0.0899\n",
            "Epoch 682/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6933, c_loss=0.0898\n",
            "Epoch 683/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6944, c_loss=0.0898\n",
            "Epoch 684/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6950, c_loss=0.0896\n",
            "Epoch 685/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6931, c_loss=0.0895\n",
            "Epoch 686/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6925, c_loss=0.0894\n",
            "Epoch 687/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6945, c_loss=0.0893\n",
            "Epoch 688/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6916, c_loss=0.0892\n",
            "Epoch 689/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6940, c_loss=0.0890\n",
            "Epoch 690/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6914, c_loss=0.0889\n",
            "Epoch 691/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6936, c_loss=0.0888\n",
            "Epoch 692/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6913, c_loss=0.0887\n",
            "Epoch 693/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6926, c_loss=0.0886\n",
            "Epoch 694/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6907, c_loss=0.0885\n",
            "Epoch 695/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.6926, c_loss=0.0884\n",
            "Epoch 696/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6904, c_loss=0.0883\n",
            "Epoch 697/2000\n",
            "Step 0: d_loss=1.3925, g_loss=0.6924, c_loss=0.0882\n",
            "Epoch 698/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6891, c_loss=0.0881\n",
            "Epoch 699/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6923, c_loss=0.0879\n",
            "Epoch 700/2000\n",
            "Step 0: d_loss=1.3929, g_loss=0.6898, c_loss=0.0878\n",
            "Epoch 701/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.6907, c_loss=0.0877\n",
            "Epoch 702/2000\n",
            "Step 0: d_loss=1.3929, g_loss=0.6920, c_loss=0.0876\n",
            "Epoch 703/2000\n",
            "Step 0: d_loss=1.3928, g_loss=0.6886, c_loss=0.0875\n",
            "Epoch 704/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.6933, c_loss=0.0874\n",
            "Epoch 705/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.6884, c_loss=0.0873\n",
            "Epoch 706/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6936, c_loss=0.0872\n",
            "Epoch 707/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6893, c_loss=0.0871\n",
            "Epoch 708/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6929, c_loss=0.0869\n",
            "Epoch 709/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6910, c_loss=0.0868\n",
            "Epoch 710/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6926, c_loss=0.0867\n",
            "Epoch 711/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6918, c_loss=0.0866\n",
            "Epoch 712/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6934, c_loss=0.0865\n",
            "Epoch 713/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6925, c_loss=0.0865\n",
            "Epoch 714/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6940, c_loss=0.0864\n",
            "Epoch 715/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6935, c_loss=0.0863\n",
            "Epoch 716/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6941, c_loss=0.0862\n",
            "Epoch 717/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6947, c_loss=0.0861\n",
            "Epoch 718/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6941, c_loss=0.0860\n",
            "Epoch 719/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6959, c_loss=0.0859\n",
            "Epoch 720/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6945, c_loss=0.0859\n",
            "Epoch 721/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6964, c_loss=0.0858\n",
            "Epoch 722/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6950, c_loss=0.0857\n",
            "Epoch 723/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6961, c_loss=0.0857\n",
            "Epoch 724/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6962, c_loss=0.0856\n",
            "Epoch 725/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6956, c_loss=0.0855\n",
            "Epoch 726/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6968, c_loss=0.0854\n",
            "Epoch 727/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6961, c_loss=0.0852\n",
            "Epoch 728/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6954, c_loss=0.0851\n",
            "Epoch 729/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6985, c_loss=0.0850\n",
            "Epoch 730/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6915, c_loss=0.0848\n",
            "Epoch 731/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.7008, c_loss=0.0847\n",
            "Epoch 732/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6918, c_loss=0.0846\n",
            "Epoch 733/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6976, c_loss=0.0845\n",
            "Epoch 734/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6955, c_loss=0.0844\n",
            "Epoch 735/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6941, c_loss=0.0843\n",
            "Epoch 736/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6983, c_loss=0.0842\n",
            "Epoch 737/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6930, c_loss=0.0841\n",
            "Epoch 738/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6968, c_loss=0.0840\n",
            "Epoch 739/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6939, c_loss=0.0839\n",
            "Epoch 740/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6973, c_loss=0.0838\n",
            "Epoch 741/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6932, c_loss=0.0837\n",
            "Epoch 742/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6963, c_loss=0.0836\n",
            "Epoch 743/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6937, c_loss=0.0835\n",
            "Epoch 744/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6975, c_loss=0.0834\n",
            "Epoch 745/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6946, c_loss=0.0833\n",
            "Epoch 746/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6959, c_loss=0.0832\n",
            "Epoch 747/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6967, c_loss=0.0831\n",
            "Epoch 748/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6951, c_loss=0.0830\n",
            "Epoch 749/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6974, c_loss=0.0829\n",
            "Epoch 750/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6935, c_loss=0.0828\n",
            "Epoch 751/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6986, c_loss=0.0827\n",
            "Epoch 752/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6934, c_loss=0.0826\n",
            "Epoch 753/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6960, c_loss=0.0825\n",
            "Epoch 754/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6952, c_loss=0.0824\n",
            "Epoch 755/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6947, c_loss=0.0823\n",
            "Epoch 756/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6929, c_loss=0.0823\n",
            "Epoch 757/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6948, c_loss=0.0822\n",
            "Epoch 758/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6906, c_loss=0.0821\n",
            "Epoch 759/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6930, c_loss=0.0820\n",
            "Epoch 760/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.6927, c_loss=0.0819\n",
            "Epoch 761/2000\n",
            "Step 0: d_loss=1.3935, g_loss=0.6876, c_loss=0.0819\n",
            "Epoch 762/2000\n",
            "Step 0: d_loss=1.3947, g_loss=0.6947, c_loss=0.0818\n",
            "Epoch 763/2000\n",
            "Step 0: d_loss=1.3952, g_loss=0.6846, c_loss=0.0817\n",
            "Epoch 764/2000\n",
            "Step 0: d_loss=1.3948, g_loss=0.6959, c_loss=0.0816\n",
            "Epoch 765/2000\n",
            "Step 0: d_loss=1.3946, g_loss=0.6832, c_loss=0.0815\n",
            "Epoch 766/2000\n",
            "Step 0: d_loss=1.3955, g_loss=0.6989, c_loss=0.0814\n",
            "Epoch 767/2000\n",
            "Step 0: d_loss=1.3939, g_loss=0.6790, c_loss=0.0813\n",
            "Epoch 768/2000\n",
            "Step 0: d_loss=1.3946, g_loss=0.7048, c_loss=0.0812\n",
            "Epoch 769/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6746, c_loss=0.0810\n",
            "Epoch 770/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.7099, c_loss=0.0809\n",
            "Epoch 771/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6725, c_loss=0.0808\n",
            "Epoch 772/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.7142, c_loss=0.0807\n",
            "Epoch 773/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6712, c_loss=0.0806\n",
            "Epoch 774/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.7176, c_loss=0.0805\n",
            "Epoch 775/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6711, c_loss=0.0804\n",
            "Epoch 776/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7208, c_loss=0.0803\n",
            "Epoch 777/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6706, c_loss=0.0803\n",
            "Epoch 778/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.7242, c_loss=0.0802\n",
            "Epoch 779/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6701, c_loss=0.0801\n",
            "Epoch 780/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.7254, c_loss=0.0800\n",
            "Epoch 781/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6698, c_loss=0.0799\n",
            "Epoch 782/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.7269, c_loss=0.0798\n",
            "Epoch 783/2000\n",
            "Step 0: d_loss=1.3793, g_loss=0.6712, c_loss=0.0797\n",
            "Epoch 784/2000\n",
            "Step 0: d_loss=1.3787, g_loss=0.7268, c_loss=0.0796\n",
            "Epoch 785/2000\n",
            "Step 0: d_loss=1.3792, g_loss=0.6725, c_loss=0.0796\n",
            "Epoch 786/2000\n",
            "Step 0: d_loss=1.3789, g_loss=0.7243, c_loss=0.0795\n",
            "Epoch 787/2000\n",
            "Step 0: d_loss=1.3795, g_loss=0.6740, c_loss=0.0794\n",
            "Epoch 788/2000\n",
            "Step 0: d_loss=1.3786, g_loss=0.7216, c_loss=0.0793\n",
            "Epoch 789/2000\n",
            "Step 0: d_loss=1.3798, g_loss=0.6770, c_loss=0.0792\n",
            "Epoch 790/2000\n",
            "Step 0: d_loss=1.3804, g_loss=0.7177, c_loss=0.0791\n",
            "Epoch 791/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6772, c_loss=0.0791\n",
            "Epoch 792/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.7152, c_loss=0.0790\n",
            "Epoch 793/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6784, c_loss=0.0789\n",
            "Epoch 794/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.7135, c_loss=0.0788\n",
            "Epoch 795/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6760, c_loss=0.0787\n",
            "Epoch 796/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.7139, c_loss=0.0786\n",
            "Epoch 797/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6732, c_loss=0.0785\n",
            "Epoch 798/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.7149, c_loss=0.0784\n",
            "Epoch 799/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6694, c_loss=0.0783\n",
            "Epoch 800/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.7175, c_loss=0.0782\n",
            "Epoch 801/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6647, c_loss=0.0782\n",
            "Epoch 802/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.7200, c_loss=0.0781\n",
            "Epoch 803/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.6615, c_loss=0.0780\n",
            "Epoch 804/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7218, c_loss=0.0779\n",
            "Epoch 805/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6579, c_loss=0.0778\n",
            "Epoch 806/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7247, c_loss=0.0777\n",
            "Epoch 807/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6566, c_loss=0.0776\n",
            "Epoch 808/2000\n",
            "Step 0: d_loss=1.3938, g_loss=0.7254, c_loss=0.0775\n",
            "Epoch 809/2000\n",
            "Step 0: d_loss=1.3929, g_loss=0.6558, c_loss=0.0775\n",
            "Epoch 810/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7261, c_loss=0.0774\n",
            "Epoch 811/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6571, c_loss=0.0773\n",
            "Epoch 812/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.7241, c_loss=0.0772\n",
            "Epoch 813/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6611, c_loss=0.0771\n",
            "Epoch 814/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.7203, c_loss=0.0770\n",
            "Epoch 815/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6673, c_loss=0.0769\n",
            "Epoch 816/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.7153, c_loss=0.0768\n",
            "Epoch 817/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6742, c_loss=0.0768\n",
            "Epoch 818/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.7098, c_loss=0.0767\n",
            "Epoch 819/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6808, c_loss=0.0766\n",
            "Epoch 820/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.7055, c_loss=0.0765\n",
            "Epoch 821/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6850, c_loss=0.0764\n",
            "Epoch 822/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.7035, c_loss=0.0764\n",
            "Epoch 823/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6889, c_loss=0.0763\n",
            "Epoch 824/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.7005, c_loss=0.0762\n",
            "Epoch 825/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6908, c_loss=0.0761\n",
            "Epoch 826/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.7004, c_loss=0.0760\n",
            "Epoch 827/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6920, c_loss=0.0759\n",
            "Epoch 828/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6987, c_loss=0.0758\n",
            "Epoch 829/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6931, c_loss=0.0757\n",
            "Epoch 830/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6969, c_loss=0.0756\n",
            "Epoch 831/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6939, c_loss=0.0755\n",
            "Epoch 832/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6968, c_loss=0.0754\n",
            "Epoch 833/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6933, c_loss=0.0753\n",
            "Epoch 834/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6961, c_loss=0.0752\n",
            "Epoch 835/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6935, c_loss=0.0751\n",
            "Epoch 836/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6954, c_loss=0.0751\n",
            "Epoch 837/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6932, c_loss=0.0750\n",
            "Epoch 838/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6954, c_loss=0.0749\n",
            "Epoch 839/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6927, c_loss=0.0748\n",
            "Epoch 840/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6953, c_loss=0.0747\n",
            "Epoch 841/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6926, c_loss=0.0747\n",
            "Epoch 842/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6950, c_loss=0.0746\n",
            "Epoch 843/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6930, c_loss=0.0745\n",
            "Epoch 844/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6948, c_loss=0.0744\n",
            "Epoch 845/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6935, c_loss=0.0744\n",
            "Epoch 846/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6946, c_loss=0.0743\n",
            "Epoch 847/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6935, c_loss=0.0742\n",
            "Epoch 848/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6950, c_loss=0.0742\n",
            "Epoch 849/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6936, c_loss=0.0741\n",
            "Epoch 850/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6953, c_loss=0.0740\n",
            "Epoch 851/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6935, c_loss=0.0739\n",
            "Epoch 852/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6959, c_loss=0.0739\n",
            "Epoch 853/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6940, c_loss=0.0738\n",
            "Epoch 854/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6963, c_loss=0.0737\n",
            "Epoch 855/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6932, c_loss=0.0736\n",
            "Epoch 856/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6967, c_loss=0.0735\n",
            "Epoch 857/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6936, c_loss=0.0734\n",
            "Epoch 858/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6966, c_loss=0.0734\n",
            "Epoch 859/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6947, c_loss=0.0733\n",
            "Epoch 860/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6953, c_loss=0.0732\n",
            "Epoch 861/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6947, c_loss=0.0731\n",
            "Epoch 862/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6956, c_loss=0.0730\n",
            "Epoch 863/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6936, c_loss=0.0730\n",
            "Epoch 864/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6960, c_loss=0.0729\n",
            "Epoch 865/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6931, c_loss=0.0728\n",
            "Epoch 866/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6960, c_loss=0.0727\n",
            "Epoch 867/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6929, c_loss=0.0726\n",
            "Epoch 868/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6950, c_loss=0.0725\n",
            "Epoch 869/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6929, c_loss=0.0724\n",
            "Epoch 870/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6942, c_loss=0.0723\n",
            "Epoch 871/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6916, c_loss=0.0722\n",
            "Epoch 872/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6948, c_loss=0.0721\n",
            "Epoch 873/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6900, c_loss=0.0721\n",
            "Epoch 874/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6951, c_loss=0.0720\n",
            "Epoch 875/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6884, c_loss=0.0719\n",
            "Epoch 876/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6950, c_loss=0.0718\n",
            "Epoch 877/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6877, c_loss=0.0717\n",
            "Epoch 878/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6957, c_loss=0.0716\n",
            "Epoch 879/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6854, c_loss=0.0716\n",
            "Epoch 880/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.6978, c_loss=0.0715\n",
            "Epoch 881/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.6836, c_loss=0.0714\n",
            "Epoch 882/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6996, c_loss=0.0713\n",
            "Epoch 883/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6815, c_loss=0.0713\n",
            "Epoch 884/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.7025, c_loss=0.0712\n",
            "Epoch 885/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6786, c_loss=0.0711\n",
            "Epoch 886/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.7068, c_loss=0.0710\n",
            "Epoch 887/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6750, c_loss=0.0710\n",
            "Epoch 888/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.7113, c_loss=0.0709\n",
            "Epoch 889/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6719, c_loss=0.0708\n",
            "Epoch 890/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.7173, c_loss=0.0707\n",
            "Epoch 891/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6669, c_loss=0.0706\n",
            "Epoch 892/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7249, c_loss=0.0706\n",
            "Epoch 893/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6609, c_loss=0.0705\n",
            "Epoch 894/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.7338, c_loss=0.0704\n",
            "Epoch 895/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6539, c_loss=0.0703\n",
            "Epoch 896/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.7427, c_loss=0.0703\n",
            "Epoch 897/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6472, c_loss=0.0702\n",
            "Epoch 898/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.7504, c_loss=0.0701\n",
            "Epoch 899/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6430, c_loss=0.0701\n",
            "Epoch 900/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.7535, c_loss=0.0700\n",
            "Epoch 901/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6428, c_loss=0.0700\n",
            "Epoch 902/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.7514, c_loss=0.0699\n",
            "Epoch 903/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6469, c_loss=0.0699\n",
            "Epoch 904/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7441, c_loss=0.0699\n",
            "Epoch 905/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6547, c_loss=0.0699\n",
            "Epoch 906/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.7332, c_loss=0.0699\n",
            "Epoch 907/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6651, c_loss=0.0699\n",
            "Epoch 908/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7221, c_loss=0.0698\n",
            "Epoch 909/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6736, c_loss=0.0697\n",
            "Epoch 910/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.7126, c_loss=0.0696\n",
            "Epoch 911/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6804, c_loss=0.0695\n",
            "Epoch 912/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.7060, c_loss=0.0693\n",
            "Epoch 913/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6845, c_loss=0.0692\n",
            "Epoch 914/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.7007, c_loss=0.0691\n",
            "Epoch 915/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6864, c_loss=0.0689\n",
            "Epoch 916/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6994, c_loss=0.0688\n",
            "Epoch 917/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6870, c_loss=0.0687\n",
            "Epoch 918/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6976, c_loss=0.0686\n",
            "Epoch 919/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6876, c_loss=0.0685\n",
            "Epoch 920/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6963, c_loss=0.0684\n",
            "Epoch 921/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6872, c_loss=0.0684\n",
            "Epoch 922/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.6965, c_loss=0.0683\n",
            "Epoch 923/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6860, c_loss=0.0682\n",
            "Epoch 924/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6968, c_loss=0.0681\n",
            "Epoch 925/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.6863, c_loss=0.0680\n",
            "Epoch 926/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6960, c_loss=0.0680\n",
            "Epoch 927/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6874, c_loss=0.0679\n",
            "Epoch 928/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6955, c_loss=0.0678\n",
            "Epoch 929/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6890, c_loss=0.0677\n",
            "Epoch 930/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6938, c_loss=0.0677\n",
            "Epoch 931/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6909, c_loss=0.0676\n",
            "Epoch 932/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6937, c_loss=0.0675\n",
            "Epoch 933/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6923, c_loss=0.0674\n",
            "Epoch 934/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6935, c_loss=0.0674\n",
            "Epoch 935/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6931, c_loss=0.0673\n",
            "Epoch 936/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6943, c_loss=0.0672\n",
            "Epoch 937/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6938, c_loss=0.0671\n",
            "Epoch 938/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6943, c_loss=0.0671\n",
            "Epoch 939/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6948, c_loss=0.0670\n",
            "Epoch 940/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6946, c_loss=0.0670\n",
            "Epoch 941/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6951, c_loss=0.0669\n",
            "Epoch 942/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6948, c_loss=0.0668\n",
            "Epoch 943/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6958, c_loss=0.0667\n",
            "Epoch 944/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6946, c_loss=0.0667\n",
            "Epoch 945/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6963, c_loss=0.0666\n",
            "Epoch 946/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6941, c_loss=0.0665\n",
            "Epoch 947/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6961, c_loss=0.0665\n",
            "Epoch 948/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6939, c_loss=0.0664\n",
            "Epoch 949/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6965, c_loss=0.0664\n",
            "Epoch 950/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6923, c_loss=0.0663\n",
            "Epoch 951/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6965, c_loss=0.0662\n",
            "Epoch 952/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6920, c_loss=0.0662\n",
            "Epoch 953/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6958, c_loss=0.0661\n",
            "Epoch 954/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6925, c_loss=0.0660\n",
            "Epoch 955/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6945, c_loss=0.0660\n",
            "Epoch 956/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6927, c_loss=0.0659\n",
            "Epoch 957/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6931, c_loss=0.0658\n",
            "Epoch 958/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6931, c_loss=0.0658\n",
            "Epoch 959/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6918, c_loss=0.0657\n",
            "Epoch 960/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6934, c_loss=0.0656\n",
            "Epoch 961/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6910, c_loss=0.0656\n",
            "Epoch 962/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6940, c_loss=0.0655\n",
            "Epoch 963/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6905, c_loss=0.0654\n",
            "Epoch 964/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6939, c_loss=0.0653\n",
            "Epoch 965/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6900, c_loss=0.0652\n",
            "Epoch 966/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6946, c_loss=0.0651\n",
            "Epoch 967/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6896, c_loss=0.0651\n",
            "Epoch 968/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6951, c_loss=0.0650\n",
            "Epoch 969/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6895, c_loss=0.0649\n",
            "Epoch 970/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6956, c_loss=0.0649\n",
            "Epoch 971/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6894, c_loss=0.0648\n",
            "Epoch 972/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6964, c_loss=0.0647\n",
            "Epoch 973/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6893, c_loss=0.0646\n",
            "Epoch 974/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6975, c_loss=0.0646\n",
            "Epoch 975/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6896, c_loss=0.0645\n",
            "Epoch 976/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6980, c_loss=0.0644\n",
            "Epoch 977/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6899, c_loss=0.0644\n",
            "Epoch 978/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6988, c_loss=0.0643\n",
            "Epoch 979/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6899, c_loss=0.0642\n",
            "Epoch 980/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.7009, c_loss=0.0641\n",
            "Epoch 981/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6885, c_loss=0.0641\n",
            "Epoch 982/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.7030, c_loss=0.0640\n",
            "Epoch 983/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6872, c_loss=0.0639\n",
            "Epoch 984/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.7045, c_loss=0.0639\n",
            "Epoch 985/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6867, c_loss=0.0638\n",
            "Epoch 986/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.7055, c_loss=0.0637\n",
            "Epoch 987/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6855, c_loss=0.0637\n",
            "Epoch 988/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.7075, c_loss=0.0636\n",
            "Epoch 989/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6837, c_loss=0.0636\n",
            "Epoch 990/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.7100, c_loss=0.0635\n",
            "Epoch 991/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6803, c_loss=0.0635\n",
            "Epoch 992/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.7125, c_loss=0.0634\n",
            "Epoch 993/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6767, c_loss=0.0633\n",
            "Epoch 994/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.7157, c_loss=0.0632\n",
            "Epoch 995/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6708, c_loss=0.0631\n",
            "Epoch 996/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.7211, c_loss=0.0631\n",
            "Epoch 997/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6629, c_loss=0.0630\n",
            "Epoch 998/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.7291, c_loss=0.0629\n",
            "Epoch 999/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6532, c_loss=0.0629\n",
            "Epoch 1000/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.7392, c_loss=0.0628\n",
            "Epoch 1001/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6423, c_loss=0.0627\n",
            "Epoch 1002/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.7495, c_loss=0.0627\n",
            "Epoch 1003/2000\n",
            "Step 0: d_loss=1.3941, g_loss=0.6315, c_loss=0.0626\n",
            "Epoch 1004/2000\n",
            "Step 0: d_loss=1.3946, g_loss=0.7587, c_loss=0.0625\n",
            "Epoch 1005/2000\n",
            "Step 0: d_loss=1.3959, g_loss=0.6255, c_loss=0.0625\n",
            "Epoch 1006/2000\n",
            "Step 0: d_loss=1.3970, g_loss=0.7613, c_loss=0.0624\n",
            "Epoch 1007/2000\n",
            "Step 0: d_loss=1.3972, g_loss=0.6265, c_loss=0.0623\n",
            "Epoch 1008/2000\n",
            "Step 0: d_loss=1.3966, g_loss=0.7529, c_loss=0.0623\n",
            "Epoch 1009/2000\n",
            "Step 0: d_loss=1.3958, g_loss=0.6397, c_loss=0.0622\n",
            "Epoch 1010/2000\n",
            "Step 0: d_loss=1.3952, g_loss=0.7347, c_loss=0.0622\n",
            "Epoch 1011/2000\n",
            "Step 0: d_loss=1.3941, g_loss=0.6576, c_loss=0.0621\n",
            "Epoch 1012/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.7159, c_loss=0.0620\n",
            "Epoch 1013/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6740, c_loss=0.0619\n",
            "Epoch 1014/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.7019, c_loss=0.0619\n",
            "Epoch 1015/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6853, c_loss=0.0618\n",
            "Epoch 1016/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6946, c_loss=0.0617\n",
            "Epoch 1017/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6905, c_loss=0.0617\n",
            "Epoch 1018/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6930, c_loss=0.0616\n",
            "Epoch 1019/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6921, c_loss=0.0615\n",
            "Epoch 1020/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6929, c_loss=0.0614\n",
            "Epoch 1021/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6931, c_loss=0.0614\n",
            "Epoch 1022/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6937, c_loss=0.0613\n",
            "Epoch 1023/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6935, c_loss=0.0613\n",
            "Epoch 1024/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6943, c_loss=0.0612\n",
            "Epoch 1025/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6933, c_loss=0.0611\n",
            "Epoch 1026/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6957, c_loss=0.0611\n",
            "Epoch 1027/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6936, c_loss=0.0611\n",
            "Epoch 1028/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6960, c_loss=0.0610\n",
            "Epoch 1029/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6940, c_loss=0.0610\n",
            "Epoch 1030/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6959, c_loss=0.0609\n",
            "Epoch 1031/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6952, c_loss=0.0608\n",
            "Epoch 1032/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6956, c_loss=0.0608\n",
            "Epoch 1033/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6959, c_loss=0.0607\n",
            "Epoch 1034/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6958, c_loss=0.0606\n",
            "Epoch 1035/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6958, c_loss=0.0606\n",
            "Epoch 1036/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6963, c_loss=0.0605\n",
            "Epoch 1037/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6957, c_loss=0.0605\n",
            "Epoch 1038/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6965, c_loss=0.0604\n",
            "Epoch 1039/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6957, c_loss=0.0603\n",
            "Epoch 1040/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6966, c_loss=0.0602\n",
            "Epoch 1041/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6954, c_loss=0.0602\n",
            "Epoch 1042/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6970, c_loss=0.0601\n",
            "Epoch 1043/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6951, c_loss=0.0600\n",
            "Epoch 1044/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6971, c_loss=0.0600\n",
            "Epoch 1045/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6947, c_loss=0.0599\n",
            "Epoch 1046/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6967, c_loss=0.0598\n",
            "Epoch 1047/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6951, c_loss=0.0598\n",
            "Epoch 1048/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6961, c_loss=0.0597\n",
            "Epoch 1049/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6958, c_loss=0.0596\n",
            "Epoch 1050/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6952, c_loss=0.0596\n",
            "Epoch 1051/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6953, c_loss=0.0595\n",
            "Epoch 1052/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6956, c_loss=0.0594\n",
            "Epoch 1053/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6941, c_loss=0.0594\n",
            "Epoch 1054/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6953, c_loss=0.0593\n",
            "Epoch 1055/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6942, c_loss=0.0592\n",
            "Epoch 1056/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6944, c_loss=0.0592\n",
            "Epoch 1057/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6945, c_loss=0.0591\n",
            "Epoch 1058/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6931, c_loss=0.0591\n",
            "Epoch 1059/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6944, c_loss=0.0590\n",
            "Epoch 1060/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6926, c_loss=0.0590\n",
            "Epoch 1061/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6938, c_loss=0.0589\n",
            "Epoch 1062/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6918, c_loss=0.0588\n",
            "Epoch 1063/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6931, c_loss=0.0588\n",
            "Epoch 1064/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6919, c_loss=0.0587\n",
            "Epoch 1065/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6921, c_loss=0.0587\n",
            "Epoch 1066/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6921, c_loss=0.0586\n",
            "Epoch 1067/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6906, c_loss=0.0585\n",
            "Epoch 1068/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6923, c_loss=0.0585\n",
            "Epoch 1069/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.6893, c_loss=0.0584\n",
            "Epoch 1070/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6930, c_loss=0.0584\n",
            "Epoch 1071/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6883, c_loss=0.0583\n",
            "Epoch 1072/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6932, c_loss=0.0582\n",
            "Epoch 1073/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6878, c_loss=0.0581\n",
            "Epoch 1074/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6932, c_loss=0.0581\n",
            "Epoch 1075/2000\n",
            "Step 0: d_loss=1.3921, g_loss=0.6882, c_loss=0.0580\n",
            "Epoch 1076/2000\n",
            "Step 0: d_loss=1.3918, g_loss=0.6928, c_loss=0.0580\n",
            "Epoch 1077/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6889, c_loss=0.0579\n",
            "Epoch 1078/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6924, c_loss=0.0579\n",
            "Epoch 1079/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6907, c_loss=0.0578\n",
            "Epoch 1080/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6918, c_loss=0.0578\n",
            "Epoch 1081/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6915, c_loss=0.0577\n",
            "Epoch 1082/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6923, c_loss=0.0576\n",
            "Epoch 1083/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6931, c_loss=0.0576\n",
            "Epoch 1084/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6920, c_loss=0.0575\n",
            "Epoch 1085/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6929, c_loss=0.0575\n",
            "Epoch 1086/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6934, c_loss=0.0574\n",
            "Epoch 1087/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6939, c_loss=0.0573\n",
            "Epoch 1088/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6945, c_loss=0.0573\n",
            "Epoch 1089/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6945, c_loss=0.0572\n",
            "Epoch 1090/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6944, c_loss=0.0571\n",
            "Epoch 1091/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6962, c_loss=0.0571\n",
            "Epoch 1092/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6953, c_loss=0.0570\n",
            "Epoch 1093/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6962, c_loss=0.0570\n",
            "Epoch 1094/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6958, c_loss=0.0569\n",
            "Epoch 1095/2000\n",
            "Step 0: d_loss=1.3813, g_loss=0.6970, c_loss=0.0569\n",
            "Epoch 1096/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6960, c_loss=0.0568\n",
            "Epoch 1097/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6966, c_loss=0.0568\n",
            "Epoch 1098/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6962, c_loss=0.0567\n",
            "Epoch 1099/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6965, c_loss=0.0567\n",
            "Epoch 1100/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6951, c_loss=0.0566\n",
            "Epoch 1101/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6970, c_loss=0.0565\n",
            "Epoch 1102/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6942, c_loss=0.0565\n",
            "Epoch 1103/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6968, c_loss=0.0564\n",
            "Epoch 1104/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6940, c_loss=0.0563\n",
            "Epoch 1105/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6952, c_loss=0.0562\n",
            "Epoch 1106/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6943, c_loss=0.0562\n",
            "Epoch 1107/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6937, c_loss=0.0561\n",
            "Epoch 1108/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6945, c_loss=0.0561\n",
            "Epoch 1109/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6921, c_loss=0.0560\n",
            "Epoch 1110/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6942, c_loss=0.0560\n",
            "Epoch 1111/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6909, c_loss=0.0559\n",
            "Epoch 1112/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6950, c_loss=0.0558\n",
            "Epoch 1113/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6886, c_loss=0.0558\n",
            "Epoch 1114/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6969, c_loss=0.0557\n",
            "Epoch 1115/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6875, c_loss=0.0557\n",
            "Epoch 1116/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6975, c_loss=0.0556\n",
            "Epoch 1117/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6843, c_loss=0.0556\n",
            "Epoch 1118/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.7003, c_loss=0.0555\n",
            "Epoch 1119/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6825, c_loss=0.0554\n",
            "Epoch 1120/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.7015, c_loss=0.0554\n",
            "Epoch 1121/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6819, c_loss=0.0553\n",
            "Epoch 1122/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.7029, c_loss=0.0553\n",
            "Epoch 1123/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6803, c_loss=0.0552\n",
            "Epoch 1124/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.7057, c_loss=0.0552\n",
            "Epoch 1125/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6777, c_loss=0.0551\n",
            "Epoch 1126/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.7095, c_loss=0.0550\n",
            "Epoch 1127/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6750, c_loss=0.0550\n",
            "Epoch 1128/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.7141, c_loss=0.0549\n",
            "Epoch 1129/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6718, c_loss=0.0549\n",
            "Epoch 1130/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.7192, c_loss=0.0548\n",
            "Epoch 1131/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6676, c_loss=0.0548\n",
            "Epoch 1132/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.7245, c_loss=0.0547\n",
            "Epoch 1133/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6644, c_loss=0.0547\n",
            "Epoch 1134/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7288, c_loss=0.0546\n",
            "Epoch 1135/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6606, c_loss=0.0546\n",
            "Epoch 1136/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.7344, c_loss=0.0545\n",
            "Epoch 1137/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6570, c_loss=0.0545\n",
            "Epoch 1138/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.7382, c_loss=0.0544\n",
            "Epoch 1139/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6543, c_loss=0.0543\n",
            "Epoch 1140/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7412, c_loss=0.0543\n",
            "Epoch 1141/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6527, c_loss=0.0542\n",
            "Epoch 1142/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.7408, c_loss=0.0542\n",
            "Epoch 1143/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6552, c_loss=0.0541\n",
            "Epoch 1144/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7377, c_loss=0.0541\n",
            "Epoch 1145/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6578, c_loss=0.0540\n",
            "Epoch 1146/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.7335, c_loss=0.0539\n",
            "Epoch 1147/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6627, c_loss=0.0539\n",
            "Epoch 1148/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7275, c_loss=0.0538\n",
            "Epoch 1149/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6676, c_loss=0.0538\n",
            "Epoch 1150/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.7214, c_loss=0.0537\n",
            "Epoch 1151/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6726, c_loss=0.0537\n",
            "Epoch 1152/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.7158, c_loss=0.0536\n",
            "Epoch 1153/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6761, c_loss=0.0536\n",
            "Epoch 1154/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.7111, c_loss=0.0535\n",
            "Epoch 1155/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6802, c_loss=0.0535\n",
            "Epoch 1156/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7060, c_loss=0.0534\n",
            "Epoch 1157/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6824, c_loss=0.0533\n",
            "Epoch 1158/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.7023, c_loss=0.0533\n",
            "Epoch 1159/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6844, c_loss=0.0532\n",
            "Epoch 1160/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6977, c_loss=0.0532\n",
            "Epoch 1161/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6860, c_loss=0.0531\n",
            "Epoch 1162/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.6966, c_loss=0.0530\n",
            "Epoch 1163/2000\n",
            "Step 0: d_loss=1.3924, g_loss=0.6853, c_loss=0.0530\n",
            "Epoch 1164/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.6955, c_loss=0.0530\n",
            "Epoch 1165/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.6842, c_loss=0.0529\n",
            "Epoch 1166/2000\n",
            "Step 0: d_loss=1.3947, g_loss=0.6956, c_loss=0.0529\n",
            "Epoch 1167/2000\n",
            "Step 0: d_loss=1.3947, g_loss=0.6831, c_loss=0.0528\n",
            "Epoch 1168/2000\n",
            "Step 0: d_loss=1.3955, g_loss=0.6962, c_loss=0.0528\n",
            "Epoch 1169/2000\n",
            "Step 0: d_loss=1.3953, g_loss=0.6815, c_loss=0.0527\n",
            "Epoch 1170/2000\n",
            "Step 0: d_loss=1.3954, g_loss=0.6970, c_loss=0.0526\n",
            "Epoch 1171/2000\n",
            "Step 0: d_loss=1.3958, g_loss=0.6814, c_loss=0.0526\n",
            "Epoch 1172/2000\n",
            "Step 0: d_loss=1.3955, g_loss=0.6968, c_loss=0.0525\n",
            "Epoch 1173/2000\n",
            "Step 0: d_loss=1.3947, g_loss=0.6816, c_loss=0.0524\n",
            "Epoch 1174/2000\n",
            "Step 0: d_loss=1.3950, g_loss=0.6979, c_loss=0.0524\n",
            "Epoch 1175/2000\n",
            "Step 0: d_loss=1.3935, g_loss=0.6814, c_loss=0.0523\n",
            "Epoch 1176/2000\n",
            "Step 0: d_loss=1.3929, g_loss=0.6995, c_loss=0.0523\n",
            "Epoch 1177/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6823, c_loss=0.0522\n",
            "Epoch 1178/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.7010, c_loss=0.0522\n",
            "Epoch 1179/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6834, c_loss=0.0521\n",
            "Epoch 1180/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.7026, c_loss=0.0521\n",
            "Epoch 1181/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6854, c_loss=0.0520\n",
            "Epoch 1182/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.7027, c_loss=0.0520\n",
            "Epoch 1183/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6879, c_loss=0.0519\n",
            "Epoch 1184/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.7032, c_loss=0.0519\n",
            "Epoch 1185/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6908, c_loss=0.0518\n",
            "Epoch 1186/2000\n",
            "Step 0: d_loss=1.3802, g_loss=0.7026, c_loss=0.0518\n",
            "Epoch 1187/2000\n",
            "Step 0: d_loss=1.3793, g_loss=0.6935, c_loss=0.0517\n",
            "Epoch 1188/2000\n",
            "Step 0: d_loss=1.3787, g_loss=0.7019, c_loss=0.0517\n",
            "Epoch 1189/2000\n",
            "Step 0: d_loss=1.3781, g_loss=0.6958, c_loss=0.0516\n",
            "Epoch 1190/2000\n",
            "Step 0: d_loss=1.3776, g_loss=0.7014, c_loss=0.0516\n",
            "Epoch 1191/2000\n",
            "Step 0: d_loss=1.3775, g_loss=0.6967, c_loss=0.0515\n",
            "Epoch 1192/2000\n",
            "Step 0: d_loss=1.3772, g_loss=0.7007, c_loss=0.0514\n",
            "Epoch 1193/2000\n",
            "Step 0: d_loss=1.3777, g_loss=0.6977, c_loss=0.0514\n",
            "Epoch 1194/2000\n",
            "Step 0: d_loss=1.3778, g_loss=0.6993, c_loss=0.0513\n",
            "Epoch 1195/2000\n",
            "Step 0: d_loss=1.3780, g_loss=0.6986, c_loss=0.0513\n",
            "Epoch 1196/2000\n",
            "Step 0: d_loss=1.3789, g_loss=0.6979, c_loss=0.0512\n",
            "Epoch 1197/2000\n",
            "Step 0: d_loss=1.3796, g_loss=0.6980, c_loss=0.0512\n",
            "Epoch 1198/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6970, c_loss=0.0511\n",
            "Epoch 1199/2000\n",
            "Step 0: d_loss=1.3811, g_loss=0.6975, c_loss=0.0511\n",
            "Epoch 1200/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6949, c_loss=0.0510\n",
            "Epoch 1201/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6973, c_loss=0.0510\n",
            "Epoch 1202/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6932, c_loss=0.0509\n",
            "Epoch 1203/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6967, c_loss=0.0509\n",
            "Epoch 1204/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6911, c_loss=0.0509\n",
            "Epoch 1205/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6967, c_loss=0.0508\n",
            "Epoch 1206/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6893, c_loss=0.0508\n",
            "Epoch 1207/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6965, c_loss=0.0507\n",
            "Epoch 1208/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6870, c_loss=0.0507\n",
            "Epoch 1209/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6980, c_loss=0.0506\n",
            "Epoch 1210/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6846, c_loss=0.0506\n",
            "Epoch 1211/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6990, c_loss=0.0505\n",
            "Epoch 1212/2000\n",
            "Step 0: d_loss=1.3928, g_loss=0.6817, c_loss=0.0505\n",
            "Epoch 1213/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.7004, c_loss=0.0504\n",
            "Epoch 1214/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.6811, c_loss=0.0504\n",
            "Epoch 1215/2000\n",
            "Step 0: d_loss=1.3928, g_loss=0.7009, c_loss=0.0503\n",
            "Epoch 1216/2000\n",
            "Step 0: d_loss=1.3932, g_loss=0.6796, c_loss=0.0502\n",
            "Epoch 1217/2000\n",
            "Step 0: d_loss=1.3938, g_loss=0.7034, c_loss=0.0502\n",
            "Epoch 1218/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6755, c_loss=0.0501\n",
            "Epoch 1219/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.7089, c_loss=0.0501\n",
            "Epoch 1220/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.6700, c_loss=0.0500\n",
            "Epoch 1221/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7155, c_loss=0.0500\n",
            "Epoch 1222/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.6640, c_loss=0.0499\n",
            "Epoch 1223/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.7230, c_loss=0.0499\n",
            "Epoch 1224/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6580, c_loss=0.0498\n",
            "Epoch 1225/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.7300, c_loss=0.0498\n",
            "Epoch 1226/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6531, c_loss=0.0497\n",
            "Epoch 1227/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.7363, c_loss=0.0497\n",
            "Epoch 1228/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6485, c_loss=0.0496\n",
            "Epoch 1229/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.7411, c_loss=0.0496\n",
            "Epoch 1230/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6467, c_loss=0.0495\n",
            "Epoch 1231/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.7425, c_loss=0.0495\n",
            "Epoch 1232/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6480, c_loss=0.0495\n",
            "Epoch 1233/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.7406, c_loss=0.0494\n",
            "Epoch 1234/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6530, c_loss=0.0494\n",
            "Epoch 1235/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7340, c_loss=0.0493\n",
            "Epoch 1236/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6615, c_loss=0.0492\n",
            "Epoch 1237/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.7249, c_loss=0.0492\n",
            "Epoch 1238/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6701, c_loss=0.0491\n",
            "Epoch 1239/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.7174, c_loss=0.0490\n",
            "Epoch 1240/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6777, c_loss=0.0490\n",
            "Epoch 1241/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.7106, c_loss=0.0490\n",
            "Epoch 1242/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6838, c_loss=0.0489\n",
            "Epoch 1243/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7049, c_loss=0.0488\n",
            "Epoch 1244/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6871, c_loss=0.0488\n",
            "Epoch 1245/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.7025, c_loss=0.0487\n",
            "Epoch 1246/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6896, c_loss=0.0487\n",
            "Epoch 1247/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.7009, c_loss=0.0486\n",
            "Epoch 1248/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6895, c_loss=0.0486\n",
            "Epoch 1249/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6992, c_loss=0.0485\n",
            "Epoch 1250/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6910, c_loss=0.0485\n",
            "Epoch 1251/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6973, c_loss=0.0485\n",
            "Epoch 1252/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6912, c_loss=0.0484\n",
            "Epoch 1253/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6964, c_loss=0.0484\n",
            "Epoch 1254/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6913, c_loss=0.0483\n",
            "Epoch 1255/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6956, c_loss=0.0483\n",
            "Epoch 1256/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6910, c_loss=0.0482\n",
            "Epoch 1257/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6950, c_loss=0.0482\n",
            "Epoch 1258/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6915, c_loss=0.0481\n",
            "Epoch 1259/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6939, c_loss=0.0481\n",
            "Epoch 1260/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6917, c_loss=0.0480\n",
            "Epoch 1261/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6930, c_loss=0.0480\n",
            "Epoch 1262/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6920, c_loss=0.0480\n",
            "Epoch 1263/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6940, c_loss=0.0479\n",
            "Epoch 1264/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6900, c_loss=0.0479\n",
            "Epoch 1265/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6955, c_loss=0.0478\n",
            "Epoch 1266/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6903, c_loss=0.0478\n",
            "Epoch 1267/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6949, c_loss=0.0477\n",
            "Epoch 1268/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6915, c_loss=0.0477\n",
            "Epoch 1269/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6937, c_loss=0.0476\n",
            "Epoch 1270/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6936, c_loss=0.0476\n",
            "Epoch 1271/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6927, c_loss=0.0476\n",
            "Epoch 1272/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6950, c_loss=0.0475\n",
            "Epoch 1273/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6927, c_loss=0.0475\n",
            "Epoch 1274/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6958, c_loss=0.0474\n",
            "Epoch 1275/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6930, c_loss=0.0474\n",
            "Epoch 1276/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6954, c_loss=0.0473\n",
            "Epoch 1277/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6945, c_loss=0.0473\n",
            "Epoch 1278/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6948, c_loss=0.0472\n",
            "Epoch 1279/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6966, c_loss=0.0472\n",
            "Epoch 1280/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6938, c_loss=0.0471\n",
            "Epoch 1281/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6982, c_loss=0.0471\n",
            "Epoch 1282/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6931, c_loss=0.0470\n",
            "Epoch 1283/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6988, c_loss=0.0470\n",
            "Epoch 1284/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6945, c_loss=0.0469\n",
            "Epoch 1285/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6967, c_loss=0.0468\n",
            "Epoch 1286/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6958, c_loss=0.0468\n",
            "Epoch 1287/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6967, c_loss=0.0467\n",
            "Epoch 1288/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6953, c_loss=0.0467\n",
            "Epoch 1289/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6976, c_loss=0.0466\n",
            "Epoch 1290/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6935, c_loss=0.0466\n",
            "Epoch 1291/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6989, c_loss=0.0465\n",
            "Epoch 1292/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6910, c_loss=0.0465\n",
            "Epoch 1293/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.7006, c_loss=0.0465\n",
            "Epoch 1294/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6885, c_loss=0.0464\n",
            "Epoch 1295/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7004, c_loss=0.0464\n",
            "Epoch 1296/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6891, c_loss=0.0463\n",
            "Epoch 1297/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6988, c_loss=0.0463\n",
            "Epoch 1298/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6873, c_loss=0.0462\n",
            "Epoch 1299/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6988, c_loss=0.0462\n",
            "Epoch 1300/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6854, c_loss=0.0462\n",
            "Epoch 1301/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6993, c_loss=0.0461\n",
            "Epoch 1302/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.6834, c_loss=0.0461\n",
            "Epoch 1303/2000\n",
            "Step 0: d_loss=1.3922, g_loss=0.6993, c_loss=0.0460\n",
            "Epoch 1304/2000\n",
            "Step 0: d_loss=1.3932, g_loss=0.6825, c_loss=0.0460\n",
            "Epoch 1305/2000\n",
            "Step 0: d_loss=1.3938, g_loss=0.6991, c_loss=0.0460\n",
            "Epoch 1306/2000\n",
            "Step 0: d_loss=1.3951, g_loss=0.6799, c_loss=0.0459\n",
            "Epoch 1307/2000\n",
            "Step 0: d_loss=1.3959, g_loss=0.7006, c_loss=0.0459\n",
            "Epoch 1308/2000\n",
            "Step 0: d_loss=1.3956, g_loss=0.6762, c_loss=0.0458\n",
            "Epoch 1309/2000\n",
            "Step 0: d_loss=1.3965, g_loss=0.7042, c_loss=0.0458\n",
            "Epoch 1310/2000\n",
            "Step 0: d_loss=1.3972, g_loss=0.6730, c_loss=0.0457\n",
            "Epoch 1311/2000\n",
            "Step 0: d_loss=1.3973, g_loss=0.7061, c_loss=0.0457\n",
            "Epoch 1312/2000\n",
            "Step 0: d_loss=1.3973, g_loss=0.6700, c_loss=0.0456\n",
            "Epoch 1313/2000\n",
            "Step 0: d_loss=1.3980, g_loss=0.7095, c_loss=0.0456\n",
            "Epoch 1314/2000\n",
            "Step 0: d_loss=1.3971, g_loss=0.6670, c_loss=0.0455\n",
            "Epoch 1315/2000\n",
            "Step 0: d_loss=1.3972, g_loss=0.7131, c_loss=0.0454\n",
            "Epoch 1316/2000\n",
            "Step 0: d_loss=1.3967, g_loss=0.6648, c_loss=0.0454\n",
            "Epoch 1317/2000\n",
            "Step 0: d_loss=1.3959, g_loss=0.7160, c_loss=0.0454\n",
            "Epoch 1318/2000\n",
            "Step 0: d_loss=1.3957, g_loss=0.6637, c_loss=0.0453\n",
            "Epoch 1319/2000\n",
            "Step 0: d_loss=1.3952, g_loss=0.7183, c_loss=0.0453\n",
            "Epoch 1320/2000\n",
            "Step 0: d_loss=1.3945, g_loss=0.6630, c_loss=0.0452\n",
            "Epoch 1321/2000\n",
            "Step 0: d_loss=1.3935, g_loss=0.7199, c_loss=0.0452\n",
            "Epoch 1322/2000\n",
            "Step 0: d_loss=1.3924, g_loss=0.6637, c_loss=0.0451\n",
            "Epoch 1323/2000\n",
            "Step 0: d_loss=1.3918, g_loss=0.7212, c_loss=0.0451\n",
            "Epoch 1324/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6651, c_loss=0.0451\n",
            "Epoch 1325/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.7208, c_loss=0.0450\n",
            "Epoch 1326/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6686, c_loss=0.0450\n",
            "Epoch 1327/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.7192, c_loss=0.0449\n",
            "Epoch 1328/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6725, c_loss=0.0449\n",
            "Epoch 1329/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7172, c_loss=0.0449\n",
            "Epoch 1330/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6768, c_loss=0.0448\n",
            "Epoch 1331/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.7149, c_loss=0.0447\n",
            "Epoch 1332/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6810, c_loss=0.0447\n",
            "Epoch 1333/2000\n",
            "Step 0: d_loss=1.3806, g_loss=0.7132, c_loss=0.0447\n",
            "Epoch 1334/2000\n",
            "Step 0: d_loss=1.3798, g_loss=0.6844, c_loss=0.0446\n",
            "Epoch 1335/2000\n",
            "Step 0: d_loss=1.3787, g_loss=0.7116, c_loss=0.0446\n",
            "Epoch 1336/2000\n",
            "Step 0: d_loss=1.3779, g_loss=0.6877, c_loss=0.0445\n",
            "Epoch 1337/2000\n",
            "Step 0: d_loss=1.3773, g_loss=0.7101, c_loss=0.0445\n",
            "Epoch 1338/2000\n",
            "Step 0: d_loss=1.3765, g_loss=0.6902, c_loss=0.0444\n",
            "Epoch 1339/2000\n",
            "Step 0: d_loss=1.3759, g_loss=0.7093, c_loss=0.0444\n",
            "Epoch 1340/2000\n",
            "Step 0: d_loss=1.3753, g_loss=0.6916, c_loss=0.0443\n",
            "Epoch 1341/2000\n",
            "Step 0: d_loss=1.3750, g_loss=0.7092, c_loss=0.0443\n",
            "Epoch 1342/2000\n",
            "Step 0: d_loss=1.3750, g_loss=0.6920, c_loss=0.0443\n",
            "Epoch 1343/2000\n",
            "Step 0: d_loss=1.3749, g_loss=0.7095, c_loss=0.0442\n",
            "Epoch 1344/2000\n",
            "Step 0: d_loss=1.3749, g_loss=0.6914, c_loss=0.0442\n",
            "Epoch 1345/2000\n",
            "Step 0: d_loss=1.3749, g_loss=0.7095, c_loss=0.0441\n",
            "Epoch 1346/2000\n",
            "Step 0: d_loss=1.3748, g_loss=0.6918, c_loss=0.0441\n",
            "Epoch 1347/2000\n",
            "Step 0: d_loss=1.3754, g_loss=0.7088, c_loss=0.0440\n",
            "Epoch 1348/2000\n",
            "Step 0: d_loss=1.3760, g_loss=0.6906, c_loss=0.0440\n",
            "Epoch 1349/2000\n",
            "Step 0: d_loss=1.3767, g_loss=0.7087, c_loss=0.0440\n",
            "Epoch 1350/2000\n",
            "Step 0: d_loss=1.3770, g_loss=0.6894, c_loss=0.0439\n",
            "Epoch 1351/2000\n",
            "Step 0: d_loss=1.3782, g_loss=0.7088, c_loss=0.0439\n",
            "Epoch 1352/2000\n",
            "Step 0: d_loss=1.3788, g_loss=0.6871, c_loss=0.0438\n",
            "Epoch 1353/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.7091, c_loss=0.0438\n",
            "Epoch 1354/2000\n",
            "Step 0: d_loss=1.3812, g_loss=0.6841, c_loss=0.0437\n",
            "Epoch 1355/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.7100, c_loss=0.0437\n",
            "Epoch 1356/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6794, c_loss=0.0437\n",
            "Epoch 1357/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.7130, c_loss=0.0436\n",
            "Epoch 1358/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6737, c_loss=0.0436\n",
            "Epoch 1359/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.7167, c_loss=0.0435\n",
            "Epoch 1360/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6656, c_loss=0.0435\n",
            "Epoch 1361/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.7234, c_loss=0.0434\n",
            "Epoch 1362/2000\n",
            "Step 0: d_loss=1.3922, g_loss=0.6560, c_loss=0.0434\n",
            "Epoch 1363/2000\n",
            "Step 0: d_loss=1.3932, g_loss=0.7328, c_loss=0.0434\n",
            "Epoch 1364/2000\n",
            "Step 0: d_loss=1.3955, g_loss=0.6437, c_loss=0.0433\n",
            "Epoch 1365/2000\n",
            "Step 0: d_loss=1.3957, g_loss=0.7430, c_loss=0.0433\n",
            "Epoch 1366/2000\n",
            "Step 0: d_loss=1.3971, g_loss=0.6333, c_loss=0.0433\n",
            "Epoch 1367/2000\n",
            "Step 0: d_loss=1.4002, g_loss=0.7538, c_loss=0.0432\n",
            "Epoch 1368/2000\n",
            "Step 0: d_loss=1.4011, g_loss=0.6203, c_loss=0.0432\n",
            "Epoch 1369/2000\n",
            "Step 0: d_loss=1.4027, g_loss=0.7633, c_loss=0.0431\n",
            "Epoch 1370/2000\n",
            "Step 0: d_loss=1.4037, g_loss=0.6158, c_loss=0.0431\n",
            "Epoch 1371/2000\n",
            "Step 0: d_loss=1.4037, g_loss=0.7605, c_loss=0.0430\n",
            "Epoch 1372/2000\n",
            "Step 0: d_loss=1.4038, g_loss=0.6250, c_loss=0.0430\n",
            "Epoch 1373/2000\n",
            "Step 0: d_loss=1.4020, g_loss=0.7420, c_loss=0.0429\n",
            "Epoch 1374/2000\n",
            "Step 0: d_loss=1.4016, g_loss=0.6459, c_loss=0.0429\n",
            "Epoch 1375/2000\n",
            "Step 0: d_loss=1.4001, g_loss=0.7179, c_loss=0.0429\n",
            "Epoch 1376/2000\n",
            "Step 0: d_loss=1.3989, g_loss=0.6670, c_loss=0.0428\n",
            "Epoch 1377/2000\n",
            "Step 0: d_loss=1.3979, g_loss=0.7000, c_loss=0.0428\n",
            "Epoch 1378/2000\n",
            "Step 0: d_loss=1.3965, g_loss=0.6808, c_loss=0.0427\n",
            "Epoch 1379/2000\n",
            "Step 0: d_loss=1.3959, g_loss=0.6916, c_loss=0.0427\n",
            "Epoch 1380/2000\n",
            "Step 0: d_loss=1.3954, g_loss=0.6878, c_loss=0.0426\n",
            "Epoch 1381/2000\n",
            "Step 0: d_loss=1.3943, g_loss=0.6883, c_loss=0.0426\n",
            "Epoch 1382/2000\n",
            "Step 0: d_loss=1.3940, g_loss=0.6904, c_loss=0.0426\n",
            "Epoch 1383/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.6885, c_loss=0.0425\n",
            "Epoch 1384/2000\n",
            "Step 0: d_loss=1.3922, g_loss=0.6909, c_loss=0.0425\n",
            "Epoch 1385/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6907, c_loss=0.0424\n",
            "Epoch 1386/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6917, c_loss=0.0424\n",
            "Epoch 1387/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6913, c_loss=0.0423\n",
            "Epoch 1388/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6931, c_loss=0.0423\n",
            "Epoch 1389/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6930, c_loss=0.0423\n",
            "Epoch 1390/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6934, c_loss=0.0422\n",
            "Epoch 1391/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6947, c_loss=0.0422\n",
            "Epoch 1392/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6938, c_loss=0.0422\n",
            "Epoch 1393/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6958, c_loss=0.0421\n",
            "Epoch 1394/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6949, c_loss=0.0421\n",
            "Epoch 1395/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6964, c_loss=0.0421\n",
            "Epoch 1396/2000\n",
            "Step 0: d_loss=1.3808, g_loss=0.6951, c_loss=0.0420\n",
            "Epoch 1397/2000\n",
            "Step 0: d_loss=1.3806, g_loss=0.6978, c_loss=0.0420\n",
            "Epoch 1398/2000\n",
            "Step 0: d_loss=1.3805, g_loss=0.6959, c_loss=0.0419\n",
            "Epoch 1399/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.6973, c_loss=0.0418\n",
            "Epoch 1400/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.6968, c_loss=0.0418\n",
            "Epoch 1401/2000\n",
            "Step 0: d_loss=1.3798, g_loss=0.6972, c_loss=0.0417\n",
            "Epoch 1402/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6968, c_loss=0.0417\n",
            "Epoch 1403/2000\n",
            "Step 0: d_loss=1.3798, g_loss=0.6971, c_loss=0.0417\n",
            "Epoch 1404/2000\n",
            "Step 0: d_loss=1.3799, g_loss=0.6966, c_loss=0.0416\n",
            "Epoch 1405/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.6975, c_loss=0.0416\n",
            "Epoch 1406/2000\n",
            "Step 0: d_loss=1.3805, g_loss=0.6961, c_loss=0.0415\n",
            "Epoch 1407/2000\n",
            "Step 0: d_loss=1.3808, g_loss=0.6975, c_loss=0.0415\n",
            "Epoch 1408/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6953, c_loss=0.0415\n",
            "Epoch 1409/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6970, c_loss=0.0414\n",
            "Epoch 1410/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6951, c_loss=0.0414\n",
            "Epoch 1411/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6960, c_loss=0.0414\n",
            "Epoch 1412/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6948, c_loss=0.0413\n",
            "Epoch 1413/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6952, c_loss=0.0413\n",
            "Epoch 1414/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6952, c_loss=0.0412\n",
            "Epoch 1415/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6941, c_loss=0.0412\n",
            "Epoch 1416/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6943, c_loss=0.0412\n",
            "Epoch 1417/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6946, c_loss=0.0411\n",
            "Epoch 1418/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6936, c_loss=0.0411\n",
            "Epoch 1419/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6943, c_loss=0.0411\n",
            "Epoch 1420/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6930, c_loss=0.0410\n",
            "Epoch 1421/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6935, c_loss=0.0410\n",
            "Epoch 1422/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6943, c_loss=0.0409\n",
            "Epoch 1423/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6918, c_loss=0.0409\n",
            "Epoch 1424/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6945, c_loss=0.0408\n",
            "Epoch 1425/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6902, c_loss=0.0408\n",
            "Epoch 1426/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6951, c_loss=0.0407\n",
            "Epoch 1427/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6912, c_loss=0.0407\n",
            "Epoch 1428/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6928, c_loss=0.0407\n",
            "Epoch 1429/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6936, c_loss=0.0406\n",
            "Epoch 1430/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6917, c_loss=0.0406\n",
            "Epoch 1431/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6932, c_loss=0.0405\n",
            "Epoch 1432/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6931, c_loss=0.0405\n",
            "Epoch 1433/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6908, c_loss=0.0405\n",
            "Epoch 1434/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6940, c_loss=0.0404\n",
            "Epoch 1435/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6916, c_loss=0.0404\n",
            "Epoch 1436/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6935, c_loss=0.0404\n",
            "Epoch 1437/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6931, c_loss=0.0403\n",
            "Epoch 1438/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6923, c_loss=0.0403\n",
            "Epoch 1439/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6953, c_loss=0.0402\n",
            "Epoch 1440/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6916, c_loss=0.0402\n",
            "Epoch 1441/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6972, c_loss=0.0401\n",
            "Epoch 1442/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6912, c_loss=0.0401\n",
            "Epoch 1443/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6971, c_loss=0.0401\n",
            "Epoch 1444/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6932, c_loss=0.0400\n",
            "Epoch 1445/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6964, c_loss=0.0400\n",
            "Epoch 1446/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6952, c_loss=0.0400\n",
            "Epoch 1447/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6958, c_loss=0.0399\n",
            "Epoch 1448/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6972, c_loss=0.0399\n",
            "Epoch 1449/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6947, c_loss=0.0399\n",
            "Epoch 1450/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6993, c_loss=0.0398\n",
            "Epoch 1451/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6946, c_loss=0.0398\n",
            "Epoch 1452/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.6998, c_loss=0.0397\n",
            "Epoch 1453/2000\n",
            "Step 0: d_loss=1.3810, g_loss=0.6951, c_loss=0.0397\n",
            "Epoch 1454/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6991, c_loss=0.0397\n",
            "Epoch 1455/2000\n",
            "Step 0: d_loss=1.3806, g_loss=0.6969, c_loss=0.0396\n",
            "Epoch 1456/2000\n",
            "Step 0: d_loss=1.3803, g_loss=0.6978, c_loss=0.0396\n",
            "Epoch 1457/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6974, c_loss=0.0395\n",
            "Epoch 1458/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6965, c_loss=0.0395\n",
            "Epoch 1459/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6971, c_loss=0.0394\n",
            "Epoch 1460/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6964, c_loss=0.0394\n",
            "Epoch 1461/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6949, c_loss=0.0393\n",
            "Epoch 1462/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6975, c_loss=0.0393\n",
            "Epoch 1463/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6920, c_loss=0.0393\n",
            "Epoch 1464/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6976, c_loss=0.0392\n",
            "Epoch 1465/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6899, c_loss=0.0392\n",
            "Epoch 1466/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6960, c_loss=0.0392\n",
            "Epoch 1467/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6901, c_loss=0.0391\n",
            "Epoch 1468/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6934, c_loss=0.0391\n",
            "Epoch 1469/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6900, c_loss=0.0391\n",
            "Epoch 1470/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.6933, c_loss=0.0390\n",
            "Epoch 1471/2000\n",
            "Step 0: d_loss=1.3938, g_loss=0.6878, c_loss=0.0390\n",
            "Epoch 1472/2000\n",
            "Step 0: d_loss=1.3917, g_loss=0.6924, c_loss=0.0390\n",
            "Epoch 1473/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.6898, c_loss=0.0389\n",
            "Epoch 1474/2000\n",
            "Step 0: d_loss=1.3924, g_loss=0.6925, c_loss=0.0389\n",
            "Epoch 1475/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.6890, c_loss=0.0388\n",
            "Epoch 1476/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6911, c_loss=0.0388\n",
            "Epoch 1477/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6909, c_loss=0.0388\n",
            "Epoch 1478/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6939, c_loss=0.0387\n",
            "Epoch 1479/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6894, c_loss=0.0387\n",
            "Epoch 1480/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6958, c_loss=0.0387\n",
            "Epoch 1481/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6906, c_loss=0.0386\n",
            "Epoch 1482/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6966, c_loss=0.0386\n",
            "Epoch 1483/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6910, c_loss=0.0385\n",
            "Epoch 1484/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6979, c_loss=0.0385\n",
            "Epoch 1485/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6929, c_loss=0.0385\n",
            "Epoch 1486/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6991, c_loss=0.0384\n",
            "Epoch 1487/2000\n",
            "Step 0: d_loss=1.3817, g_loss=0.6923, c_loss=0.0384\n",
            "Epoch 1488/2000\n",
            "Step 0: d_loss=1.3812, g_loss=0.7001, c_loss=0.0383\n",
            "Epoch 1489/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.6934, c_loss=0.0383\n",
            "Epoch 1490/2000\n",
            "Step 0: d_loss=1.3804, g_loss=0.6982, c_loss=0.0382\n",
            "Epoch 1491/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6959, c_loss=0.0382\n",
            "Epoch 1492/2000\n",
            "Step 0: d_loss=1.3811, g_loss=0.6992, c_loss=0.0382\n",
            "Epoch 1493/2000\n",
            "Step 0: d_loss=1.3803, g_loss=0.6929, c_loss=0.0381\n",
            "Epoch 1494/2000\n",
            "Step 0: d_loss=1.3814, g_loss=0.7018, c_loss=0.0381\n",
            "Epoch 1495/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6907, c_loss=0.0381\n",
            "Epoch 1496/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.7009, c_loss=0.0380\n",
            "Epoch 1497/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6919, c_loss=0.0380\n",
            "Epoch 1498/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6991, c_loss=0.0380\n",
            "Epoch 1499/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6922, c_loss=0.0379\n",
            "Epoch 1500/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6956, c_loss=0.0379\n",
            "Epoch 1501/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6949, c_loss=0.0379\n",
            "Epoch 1502/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6938, c_loss=0.0379\n",
            "Epoch 1503/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6939, c_loss=0.0378\n",
            "Epoch 1504/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6946, c_loss=0.0378\n",
            "Epoch 1505/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.6932, c_loss=0.0378\n",
            "Epoch 1506/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6938, c_loss=0.0377\n",
            "Epoch 1507/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6928, c_loss=0.0377\n",
            "Epoch 1508/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6942, c_loss=0.0376\n",
            "Epoch 1509/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6912, c_loss=0.0376\n",
            "Epoch 1510/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6945, c_loss=0.0376\n",
            "Epoch 1511/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6902, c_loss=0.0375\n",
            "Epoch 1512/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6951, c_loss=0.0375\n",
            "Epoch 1513/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6897, c_loss=0.0374\n",
            "Epoch 1514/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6950, c_loss=0.0374\n",
            "Epoch 1515/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6889, c_loss=0.0374\n",
            "Epoch 1516/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6962, c_loss=0.0373\n",
            "Epoch 1517/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6871, c_loss=0.0373\n",
            "Epoch 1518/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6973, c_loss=0.0373\n",
            "Epoch 1519/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6860, c_loss=0.0372\n",
            "Epoch 1520/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6982, c_loss=0.0372\n",
            "Epoch 1521/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6847, c_loss=0.0371\n",
            "Epoch 1522/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6996, c_loss=0.0371\n",
            "Epoch 1523/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6835, c_loss=0.0371\n",
            "Epoch 1524/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.7011, c_loss=0.0370\n",
            "Epoch 1525/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6826, c_loss=0.0370\n",
            "Epoch 1526/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.7019, c_loss=0.0370\n",
            "Epoch 1527/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6823, c_loss=0.0369\n",
            "Epoch 1528/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.7023, c_loss=0.0369\n",
            "Epoch 1529/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6824, c_loss=0.0369\n",
            "Epoch 1530/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.7026, c_loss=0.0368\n",
            "Epoch 1531/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6816, c_loss=0.0368\n",
            "Epoch 1532/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.7050, c_loss=0.0367\n",
            "Epoch 1533/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6801, c_loss=0.0367\n",
            "Epoch 1534/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.7077, c_loss=0.0367\n",
            "Epoch 1535/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6783, c_loss=0.0366\n",
            "Epoch 1536/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.7099, c_loss=0.0366\n",
            "Epoch 1537/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6774, c_loss=0.0365\n",
            "Epoch 1538/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.7110, c_loss=0.0365\n",
            "Epoch 1539/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6774, c_loss=0.0365\n",
            "Epoch 1540/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.7121, c_loss=0.0365\n",
            "Epoch 1541/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6772, c_loss=0.0364\n",
            "Epoch 1542/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7134, c_loss=0.0364\n",
            "Epoch 1543/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6760, c_loss=0.0363\n",
            "Epoch 1544/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.7152, c_loss=0.0363\n",
            "Epoch 1545/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6754, c_loss=0.0363\n",
            "Epoch 1546/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.7169, c_loss=0.0363\n",
            "Epoch 1547/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6740, c_loss=0.0362\n",
            "Epoch 1548/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.7188, c_loss=0.0362\n",
            "Epoch 1549/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6728, c_loss=0.0362\n",
            "Epoch 1550/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.7201, c_loss=0.0361\n",
            "Epoch 1551/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6723, c_loss=0.0361\n",
            "Epoch 1552/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.7207, c_loss=0.0360\n",
            "Epoch 1553/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6718, c_loss=0.0360\n",
            "Epoch 1554/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.7210, c_loss=0.0360\n",
            "Epoch 1555/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6718, c_loss=0.0359\n",
            "Epoch 1556/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.7208, c_loss=0.0359\n",
            "Epoch 1557/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6721, c_loss=0.0359\n",
            "Epoch 1558/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.7204, c_loss=0.0358\n",
            "Epoch 1559/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6709, c_loss=0.0358\n",
            "Epoch 1560/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.7221, c_loss=0.0358\n",
            "Epoch 1561/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6697, c_loss=0.0357\n",
            "Epoch 1562/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.7218, c_loss=0.0357\n",
            "Epoch 1563/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6698, c_loss=0.0356\n",
            "Epoch 1564/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.7214, c_loss=0.0356\n",
            "Epoch 1565/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6698, c_loss=0.0356\n",
            "Epoch 1566/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.7198, c_loss=0.0355\n",
            "Epoch 1567/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6712, c_loss=0.0355\n",
            "Epoch 1568/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.7174, c_loss=0.0355\n",
            "Epoch 1569/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6710, c_loss=0.0355\n",
            "Epoch 1570/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.7167, c_loss=0.0354\n",
            "Epoch 1571/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6721, c_loss=0.0354\n",
            "Epoch 1572/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.7145, c_loss=0.0354\n",
            "Epoch 1573/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6718, c_loss=0.0353\n",
            "Epoch 1574/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.7158, c_loss=0.0353\n",
            "Epoch 1575/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6702, c_loss=0.0352\n",
            "Epoch 1576/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.7151, c_loss=0.0352\n",
            "Epoch 1577/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6693, c_loss=0.0351\n",
            "Epoch 1578/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.7159, c_loss=0.0351\n",
            "Epoch 1579/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6685, c_loss=0.0351\n",
            "Epoch 1580/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.7141, c_loss=0.0350\n",
            "Epoch 1581/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6697, c_loss=0.0350\n",
            "Epoch 1582/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.7138, c_loss=0.0350\n",
            "Epoch 1583/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6687, c_loss=0.0349\n",
            "Epoch 1584/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.7141, c_loss=0.0349\n",
            "Epoch 1585/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6692, c_loss=0.0349\n",
            "Epoch 1586/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.7136, c_loss=0.0349\n",
            "Epoch 1587/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.6693, c_loss=0.0348\n",
            "Epoch 1588/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.7123, c_loss=0.0348\n",
            "Epoch 1589/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6721, c_loss=0.0348\n",
            "Epoch 1590/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.7093, c_loss=0.0348\n",
            "Epoch 1591/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6760, c_loss=0.0347\n",
            "Epoch 1592/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.7062, c_loss=0.0347\n",
            "Epoch 1593/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6798, c_loss=0.0347\n",
            "Epoch 1594/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.7039, c_loss=0.0347\n",
            "Epoch 1595/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6822, c_loss=0.0346\n",
            "Epoch 1596/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.7027, c_loss=0.0346\n",
            "Epoch 1597/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6849, c_loss=0.0345\n",
            "Epoch 1598/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.7009, c_loss=0.0345\n",
            "Epoch 1599/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6871, c_loss=0.0345\n",
            "Epoch 1600/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.7007, c_loss=0.0344\n",
            "Epoch 1601/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6883, c_loss=0.0344\n",
            "Epoch 1602/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.7004, c_loss=0.0344\n",
            "Epoch 1603/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6895, c_loss=0.0343\n",
            "Epoch 1604/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6997, c_loss=0.0343\n",
            "Epoch 1605/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6906, c_loss=0.0343\n",
            "Epoch 1606/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6996, c_loss=0.0342\n",
            "Epoch 1607/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6912, c_loss=0.0342\n",
            "Epoch 1608/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6992, c_loss=0.0341\n",
            "Epoch 1609/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6913, c_loss=0.0341\n",
            "Epoch 1610/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6992, c_loss=0.0341\n",
            "Epoch 1611/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6918, c_loss=0.0340\n",
            "Epoch 1612/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6982, c_loss=0.0340\n",
            "Epoch 1613/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6920, c_loss=0.0340\n",
            "Epoch 1614/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.6970, c_loss=0.0339\n",
            "Epoch 1615/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6926, c_loss=0.0339\n",
            "Epoch 1616/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6959, c_loss=0.0339\n",
            "Epoch 1617/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6925, c_loss=0.0338\n",
            "Epoch 1618/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6953, c_loss=0.0338\n",
            "Epoch 1619/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6915, c_loss=0.0338\n",
            "Epoch 1620/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6956, c_loss=0.0337\n",
            "Epoch 1621/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6903, c_loss=0.0337\n",
            "Epoch 1622/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6961, c_loss=0.0337\n",
            "Epoch 1623/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6887, c_loss=0.0337\n",
            "Epoch 1624/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6969, c_loss=0.0336\n",
            "Epoch 1625/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6877, c_loss=0.0336\n",
            "Epoch 1626/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6965, c_loss=0.0335\n",
            "Epoch 1627/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6869, c_loss=0.0335\n",
            "Epoch 1628/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6970, c_loss=0.0335\n",
            "Epoch 1629/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6858, c_loss=0.0334\n",
            "Epoch 1630/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6974, c_loss=0.0334\n",
            "Epoch 1631/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6848, c_loss=0.0334\n",
            "Epoch 1632/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6994, c_loss=0.0333\n",
            "Epoch 1633/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.6828, c_loss=0.0333\n",
            "Epoch 1634/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.7010, c_loss=0.0333\n",
            "Epoch 1635/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6816, c_loss=0.0333\n",
            "Epoch 1636/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.7023, c_loss=0.0332\n",
            "Epoch 1637/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6809, c_loss=0.0332\n",
            "Epoch 1638/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.7035, c_loss=0.0332\n",
            "Epoch 1639/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6808, c_loss=0.0331\n",
            "Epoch 1640/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.7044, c_loss=0.0331\n",
            "Epoch 1641/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6807, c_loss=0.0331\n",
            "Epoch 1642/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.7060, c_loss=0.0330\n",
            "Epoch 1643/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6800, c_loss=0.0330\n",
            "Epoch 1644/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.7077, c_loss=0.0330\n",
            "Epoch 1645/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6797, c_loss=0.0330\n",
            "Epoch 1646/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.7101, c_loss=0.0329\n",
            "Epoch 1647/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.6788, c_loss=0.0329\n",
            "Epoch 1648/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7117, c_loss=0.0329\n",
            "Epoch 1649/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6785, c_loss=0.0329\n",
            "Epoch 1650/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.7138, c_loss=0.0328\n",
            "Epoch 1651/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6776, c_loss=0.0328\n",
            "Epoch 1652/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.7163, c_loss=0.0328\n",
            "Epoch 1653/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6763, c_loss=0.0327\n",
            "Epoch 1654/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.7184, c_loss=0.0327\n",
            "Epoch 1655/2000\n",
            "Step 0: d_loss=1.3812, g_loss=0.6751, c_loss=0.0326\n",
            "Epoch 1656/2000\n",
            "Step 0: d_loss=1.3811, g_loss=0.7205, c_loss=0.0326\n",
            "Epoch 1657/2000\n",
            "Step 0: d_loss=1.3809, g_loss=0.6739, c_loss=0.0326\n",
            "Epoch 1658/2000\n",
            "Step 0: d_loss=1.3804, g_loss=0.7222, c_loss=0.0325\n",
            "Epoch 1659/2000\n",
            "Step 0: d_loss=1.3803, g_loss=0.6733, c_loss=0.0325\n",
            "Epoch 1660/2000\n",
            "Step 0: d_loss=1.3802, g_loss=0.7230, c_loss=0.0325\n",
            "Epoch 1661/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6726, c_loss=0.0325\n",
            "Epoch 1662/2000\n",
            "Step 0: d_loss=1.3804, g_loss=0.7237, c_loss=0.0324\n",
            "Epoch 1663/2000\n",
            "Step 0: d_loss=1.3805, g_loss=0.6715, c_loss=0.0324\n",
            "Epoch 1664/2000\n",
            "Step 0: d_loss=1.3808, g_loss=0.7249, c_loss=0.0324\n",
            "Epoch 1665/2000\n",
            "Step 0: d_loss=1.3810, g_loss=0.6696, c_loss=0.0323\n",
            "Epoch 1666/2000\n",
            "Step 0: d_loss=1.3816, g_loss=0.7263, c_loss=0.0323\n",
            "Epoch 1667/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6664, c_loss=0.0323\n",
            "Epoch 1668/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.7293, c_loss=0.0323\n",
            "Epoch 1669/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6625, c_loss=0.0322\n",
            "Epoch 1670/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.7322, c_loss=0.0322\n",
            "Epoch 1671/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.6580, c_loss=0.0321\n",
            "Epoch 1672/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.7362, c_loss=0.0321\n",
            "Epoch 1673/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6535, c_loss=0.0321\n",
            "Epoch 1674/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.7385, c_loss=0.0321\n",
            "Epoch 1675/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6498, c_loss=0.0320\n",
            "Epoch 1676/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.7408, c_loss=0.0320\n",
            "Epoch 1677/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6468, c_loss=0.0320\n",
            "Epoch 1678/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.7405, c_loss=0.0320\n",
            "Epoch 1679/2000\n",
            "Step 0: d_loss=1.3912, g_loss=0.6470, c_loss=0.0319\n",
            "Epoch 1680/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.7373, c_loss=0.0319\n",
            "Epoch 1681/2000\n",
            "Step 0: d_loss=1.3926, g_loss=0.6490, c_loss=0.0319\n",
            "Epoch 1682/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.7324, c_loss=0.0318\n",
            "Epoch 1683/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6537, c_loss=0.0318\n",
            "Epoch 1684/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.7257, c_loss=0.0318\n",
            "Epoch 1685/2000\n",
            "Step 0: d_loss=1.3945, g_loss=0.6592, c_loss=0.0317\n",
            "Epoch 1686/2000\n",
            "Step 0: d_loss=1.3943, g_loss=0.7176, c_loss=0.0317\n",
            "Epoch 1687/2000\n",
            "Step 0: d_loss=1.3943, g_loss=0.6665, c_loss=0.0316\n",
            "Epoch 1688/2000\n",
            "Step 0: d_loss=1.3947, g_loss=0.7088, c_loss=0.0316\n",
            "Epoch 1689/2000\n",
            "Step 0: d_loss=1.3948, g_loss=0.6737, c_loss=0.0316\n",
            "Epoch 1690/2000\n",
            "Step 0: d_loss=1.3943, g_loss=0.7016, c_loss=0.0315\n",
            "Epoch 1691/2000\n",
            "Step 0: d_loss=1.3941, g_loss=0.6798, c_loss=0.0315\n",
            "Epoch 1692/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.6967, c_loss=0.0315\n",
            "Epoch 1693/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.6834, c_loss=0.0315\n",
            "Epoch 1694/2000\n",
            "Step 0: d_loss=1.3944, g_loss=0.6939, c_loss=0.0315\n",
            "Epoch 1695/2000\n",
            "Step 0: d_loss=1.3939, g_loss=0.6849, c_loss=0.0314\n",
            "Epoch 1696/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6931, c_loss=0.0314\n",
            "Epoch 1697/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.6873, c_loss=0.0314\n",
            "Epoch 1698/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6915, c_loss=0.0313\n",
            "Epoch 1699/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.6892, c_loss=0.0313\n",
            "Epoch 1700/2000\n",
            "Step 0: d_loss=1.3918, g_loss=0.6906, c_loss=0.0313\n",
            "Epoch 1701/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6911, c_loss=0.0312\n",
            "Epoch 1702/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6903, c_loss=0.0312\n",
            "Epoch 1703/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6925, c_loss=0.0312\n",
            "Epoch 1704/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6908, c_loss=0.0311\n",
            "Epoch 1705/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6933, c_loss=0.0311\n",
            "Epoch 1706/2000\n",
            "Step 0: d_loss=1.3872, g_loss=0.6921, c_loss=0.0311\n",
            "Epoch 1707/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6935, c_loss=0.0311\n",
            "Epoch 1708/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6933, c_loss=0.0310\n",
            "Epoch 1709/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6942, c_loss=0.0310\n",
            "Epoch 1710/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6941, c_loss=0.0310\n",
            "Epoch 1711/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6952, c_loss=0.0310\n",
            "Epoch 1712/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6947, c_loss=0.0309\n",
            "Epoch 1713/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6960, c_loss=0.0309\n",
            "Epoch 1714/2000\n",
            "Step 0: d_loss=1.3813, g_loss=0.6951, c_loss=0.0309\n",
            "Epoch 1715/2000\n",
            "Step 0: d_loss=1.3813, g_loss=0.6971, c_loss=0.0308\n",
            "Epoch 1716/2000\n",
            "Step 0: d_loss=1.3807, g_loss=0.6951, c_loss=0.0308\n",
            "Epoch 1717/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6975, c_loss=0.0308\n",
            "Epoch 1718/2000\n",
            "Step 0: d_loss=1.3801, g_loss=0.6965, c_loss=0.0307\n",
            "Epoch 1719/2000\n",
            "Step 0: d_loss=1.3798, g_loss=0.6964, c_loss=0.0307\n",
            "Epoch 1720/2000\n",
            "Step 0: d_loss=1.3795, g_loss=0.6976, c_loss=0.0307\n",
            "Epoch 1721/2000\n",
            "Step 0: d_loss=1.3792, g_loss=0.6967, c_loss=0.0306\n",
            "Epoch 1722/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6972, c_loss=0.0306\n",
            "Epoch 1723/2000\n",
            "Step 0: d_loss=1.3795, g_loss=0.6972, c_loss=0.0306\n",
            "Epoch 1724/2000\n",
            "Step 0: d_loss=1.3797, g_loss=0.6961, c_loss=0.0306\n",
            "Epoch 1725/2000\n",
            "Step 0: d_loss=1.3800, g_loss=0.6982, c_loss=0.0306\n",
            "Epoch 1726/2000\n",
            "Step 0: d_loss=1.3803, g_loss=0.6950, c_loss=0.0305\n",
            "Epoch 1727/2000\n",
            "Step 0: d_loss=1.3805, g_loss=0.6981, c_loss=0.0305\n",
            "Epoch 1728/2000\n",
            "Step 0: d_loss=1.3812, g_loss=0.6951, c_loss=0.0305\n",
            "Epoch 1729/2000\n",
            "Step 0: d_loss=1.3815, g_loss=0.6970, c_loss=0.0305\n",
            "Epoch 1730/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6951, c_loss=0.0304\n",
            "Epoch 1731/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6959, c_loss=0.0304\n",
            "Epoch 1732/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6954, c_loss=0.0304\n",
            "Epoch 1733/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.6950, c_loss=0.0303\n",
            "Epoch 1734/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6952, c_loss=0.0303\n",
            "Epoch 1735/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6940, c_loss=0.0302\n",
            "Epoch 1736/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6949, c_loss=0.0302\n",
            "Epoch 1737/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6932, c_loss=0.0302\n",
            "Epoch 1738/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6955, c_loss=0.0301\n",
            "Epoch 1739/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6921, c_loss=0.0301\n",
            "Epoch 1740/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6954, c_loss=0.0301\n",
            "Epoch 1741/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6913, c_loss=0.0301\n",
            "Epoch 1742/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6959, c_loss=0.0300\n",
            "Epoch 1743/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.6910, c_loss=0.0300\n",
            "Epoch 1744/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6946, c_loss=0.0300\n",
            "Epoch 1745/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6912, c_loss=0.0300\n",
            "Epoch 1746/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6955, c_loss=0.0299\n",
            "Epoch 1747/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6901, c_loss=0.0299\n",
            "Epoch 1748/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.6946, c_loss=0.0299\n",
            "Epoch 1749/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6914, c_loss=0.0299\n",
            "Epoch 1750/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6933, c_loss=0.0298\n",
            "Epoch 1751/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6917, c_loss=0.0298\n",
            "Epoch 1752/2000\n",
            "Step 0: d_loss=1.3893, g_loss=0.6924, c_loss=0.0297\n",
            "Epoch 1753/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6923, c_loss=0.0297\n",
            "Epoch 1754/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6927, c_loss=0.0297\n",
            "Epoch 1755/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6917, c_loss=0.0297\n",
            "Epoch 1756/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6930, c_loss=0.0296\n",
            "Epoch 1757/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6918, c_loss=0.0296\n",
            "Epoch 1758/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6922, c_loss=0.0296\n",
            "Epoch 1759/2000\n",
            "Step 0: d_loss=1.3902, g_loss=0.6922, c_loss=0.0296\n",
            "Epoch 1760/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6915, c_loss=0.0295\n",
            "Epoch 1761/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.6927, c_loss=0.0295\n",
            "Epoch 1762/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6924, c_loss=0.0295\n",
            "Epoch 1763/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6908, c_loss=0.0294\n",
            "Epoch 1764/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6950, c_loss=0.0294\n",
            "Epoch 1765/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6886, c_loss=0.0294\n",
            "Epoch 1766/2000\n",
            "Step 0: d_loss=1.3896, g_loss=0.6954, c_loss=0.0294\n",
            "Epoch 1767/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6897, c_loss=0.0293\n",
            "Epoch 1768/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6955, c_loss=0.0293\n",
            "Epoch 1769/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6890, c_loss=0.0293\n",
            "Epoch 1770/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6957, c_loss=0.0293\n",
            "Epoch 1771/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6884, c_loss=0.0292\n",
            "Epoch 1772/2000\n",
            "Step 0: d_loss=1.3892, g_loss=0.6972, c_loss=0.0292\n",
            "Epoch 1773/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6876, c_loss=0.0292\n",
            "Epoch 1774/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6976, c_loss=0.0292\n",
            "Epoch 1775/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6875, c_loss=0.0291\n",
            "Epoch 1776/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6980, c_loss=0.0291\n",
            "Epoch 1777/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6873, c_loss=0.0291\n",
            "Epoch 1778/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.6982, c_loss=0.0291\n",
            "Epoch 1779/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6873, c_loss=0.0290\n",
            "Epoch 1780/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6989, c_loss=0.0290\n",
            "Epoch 1781/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6869, c_loss=0.0290\n",
            "Epoch 1782/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.6995, c_loss=0.0289\n",
            "Epoch 1783/2000\n",
            "Step 0: d_loss=1.3878, g_loss=0.6866, c_loss=0.0289\n",
            "Epoch 1784/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6998, c_loss=0.0289\n",
            "Epoch 1785/2000\n",
            "Step 0: d_loss=1.3880, g_loss=0.6865, c_loss=0.0288\n",
            "Epoch 1786/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.7003, c_loss=0.0288\n",
            "Epoch 1787/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6865, c_loss=0.0288\n",
            "Epoch 1788/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.7007, c_loss=0.0288\n",
            "Epoch 1789/2000\n",
            "Step 0: d_loss=1.3868, g_loss=0.6864, c_loss=0.0287\n",
            "Epoch 1790/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.7011, c_loss=0.0287\n",
            "Epoch 1791/2000\n",
            "Step 0: d_loss=1.3866, g_loss=0.6864, c_loss=0.0287\n",
            "Epoch 1792/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.7016, c_loss=0.0286\n",
            "Epoch 1793/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6860, c_loss=0.0286\n",
            "Epoch 1794/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.7022, c_loss=0.0286\n",
            "Epoch 1795/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6855, c_loss=0.0286\n",
            "Epoch 1796/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.7035, c_loss=0.0285\n",
            "Epoch 1797/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6846, c_loss=0.0285\n",
            "Epoch 1798/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7047, c_loss=0.0285\n",
            "Epoch 1799/2000\n",
            "Step 0: d_loss=1.3851, g_loss=0.6835, c_loss=0.0284\n",
            "Epoch 1800/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7066, c_loss=0.0284\n",
            "Epoch 1801/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6818, c_loss=0.0284\n",
            "Epoch 1802/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.7082, c_loss=0.0284\n",
            "Epoch 1803/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6810, c_loss=0.0283\n",
            "Epoch 1804/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.7092, c_loss=0.0283\n",
            "Epoch 1805/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6806, c_loss=0.0283\n",
            "Epoch 1806/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.7099, c_loss=0.0283\n",
            "Epoch 1807/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6799, c_loss=0.0283\n",
            "Epoch 1808/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.7111, c_loss=0.0282\n",
            "Epoch 1809/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6792, c_loss=0.0282\n",
            "Epoch 1810/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.7119, c_loss=0.0282\n",
            "Epoch 1811/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6792, c_loss=0.0282\n",
            "Epoch 1812/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.7123, c_loss=0.0282\n",
            "Epoch 1813/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6781, c_loss=0.0281\n",
            "Epoch 1814/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.7143, c_loss=0.0281\n",
            "Epoch 1815/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6763, c_loss=0.0281\n",
            "Epoch 1816/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.7162, c_loss=0.0280\n",
            "Epoch 1817/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6746, c_loss=0.0280\n",
            "Epoch 1818/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.7181, c_loss=0.0280\n",
            "Epoch 1819/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6727, c_loss=0.0279\n",
            "Epoch 1820/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.7196, c_loss=0.0279\n",
            "Epoch 1821/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6706, c_loss=0.0279\n",
            "Epoch 1822/2000\n",
            "Step 0: d_loss=1.3835, g_loss=0.7226, c_loss=0.0278\n",
            "Epoch 1823/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6666, c_loss=0.0278\n",
            "Epoch 1824/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.7265, c_loss=0.0278\n",
            "Epoch 1825/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6622, c_loss=0.0278\n",
            "Epoch 1826/2000\n",
            "Step 0: d_loss=1.3853, g_loss=0.7298, c_loss=0.0277\n",
            "Epoch 1827/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6589, c_loss=0.0277\n",
            "Epoch 1828/2000\n",
            "Step 0: d_loss=1.3861, g_loss=0.7325, c_loss=0.0277\n",
            "Epoch 1829/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6561, c_loss=0.0277\n",
            "Epoch 1830/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.7339, c_loss=0.0276\n",
            "Epoch 1831/2000\n",
            "Step 0: d_loss=1.3877, g_loss=0.6541, c_loss=0.0276\n",
            "Epoch 1832/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.7346, c_loss=0.0276\n",
            "Epoch 1833/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6532, c_loss=0.0276\n",
            "Epoch 1834/2000\n",
            "Step 0: d_loss=1.3895, g_loss=0.7340, c_loss=0.0275\n",
            "Epoch 1835/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6529, c_loss=0.0275\n",
            "Epoch 1836/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.7322, c_loss=0.0275\n",
            "Epoch 1837/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6546, c_loss=0.0275\n",
            "Epoch 1838/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.7278, c_loss=0.0274\n",
            "Epoch 1839/2000\n",
            "Step 0: d_loss=1.3920, g_loss=0.6587, c_loss=0.0274\n",
            "Epoch 1840/2000\n",
            "Step 0: d_loss=1.3923, g_loss=0.7215, c_loss=0.0274\n",
            "Epoch 1841/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.6640, c_loss=0.0274\n",
            "Epoch 1842/2000\n",
            "Step 0: d_loss=1.3930, g_loss=0.7146, c_loss=0.0273\n",
            "Epoch 1843/2000\n",
            "Step 0: d_loss=1.3933, g_loss=0.6690, c_loss=0.0273\n",
            "Epoch 1844/2000\n",
            "Step 0: d_loss=1.3932, g_loss=0.7094, c_loss=0.0273\n",
            "Epoch 1845/2000\n",
            "Step 0: d_loss=1.3934, g_loss=0.6733, c_loss=0.0272\n",
            "Epoch 1846/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.7050, c_loss=0.0272\n",
            "Epoch 1847/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.6765, c_loss=0.0272\n",
            "Epoch 1848/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.7020, c_loss=0.0272\n",
            "Epoch 1849/2000\n",
            "Step 0: d_loss=1.3937, g_loss=0.6793, c_loss=0.0272\n",
            "Epoch 1850/2000\n",
            "Step 0: d_loss=1.3935, g_loss=0.6990, c_loss=0.0271\n",
            "Epoch 1851/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6825, c_loss=0.0271\n",
            "Epoch 1852/2000\n",
            "Step 0: d_loss=1.3931, g_loss=0.6964, c_loss=0.0271\n",
            "Epoch 1853/2000\n",
            "Step 0: d_loss=1.3927, g_loss=0.6851, c_loss=0.0270\n",
            "Epoch 1854/2000\n",
            "Step 0: d_loss=1.3921, g_loss=0.6949, c_loss=0.0270\n",
            "Epoch 1855/2000\n",
            "Step 0: d_loss=1.3919, g_loss=0.6875, c_loss=0.0270\n",
            "Epoch 1856/2000\n",
            "Step 0: d_loss=1.3913, g_loss=0.6934, c_loss=0.0269\n",
            "Epoch 1857/2000\n",
            "Step 0: d_loss=1.3907, g_loss=0.6898, c_loss=0.0269\n",
            "Epoch 1858/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6931, c_loss=0.0269\n",
            "Epoch 1859/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6908, c_loss=0.0269\n",
            "Epoch 1860/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.6931, c_loss=0.0268\n",
            "Epoch 1861/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6924, c_loss=0.0268\n",
            "Epoch 1862/2000\n",
            "Step 0: d_loss=1.3874, g_loss=0.6932, c_loss=0.0268\n",
            "Epoch 1863/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.6936, c_loss=0.0268\n",
            "Epoch 1864/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6932, c_loss=0.0267\n",
            "Epoch 1865/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6952, c_loss=0.0267\n",
            "Epoch 1866/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6931, c_loss=0.0267\n",
            "Epoch 1867/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6960, c_loss=0.0266\n",
            "Epoch 1868/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6935, c_loss=0.0266\n",
            "Epoch 1869/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6969, c_loss=0.0266\n",
            "Epoch 1870/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6933, c_loss=0.0266\n",
            "Epoch 1871/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6976, c_loss=0.0266\n",
            "Epoch 1872/2000\n",
            "Step 0: d_loss=1.3825, g_loss=0.6941, c_loss=0.0266\n",
            "Epoch 1873/2000\n",
            "Step 0: d_loss=1.3826, g_loss=0.6973, c_loss=0.0265\n",
            "Epoch 1874/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6945, c_loss=0.0265\n",
            "Epoch 1875/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6974, c_loss=0.0265\n",
            "Epoch 1876/2000\n",
            "Step 0: d_loss=1.3818, g_loss=0.6945, c_loss=0.0264\n",
            "Epoch 1877/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6978, c_loss=0.0264\n",
            "Epoch 1878/2000\n",
            "Step 0: d_loss=1.3819, g_loss=0.6941, c_loss=0.0264\n",
            "Epoch 1879/2000\n",
            "Step 0: d_loss=1.3824, g_loss=0.6980, c_loss=0.0263\n",
            "Epoch 1880/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6942, c_loss=0.0263\n",
            "Epoch 1881/2000\n",
            "Step 0: d_loss=1.3821, g_loss=0.6972, c_loss=0.0263\n",
            "Epoch 1882/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6955, c_loss=0.0263\n",
            "Epoch 1883/2000\n",
            "Step 0: d_loss=1.3822, g_loss=0.6954, c_loss=0.0262\n",
            "Epoch 1884/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6970, c_loss=0.0262\n",
            "Epoch 1885/2000\n",
            "Step 0: d_loss=1.3827, g_loss=0.6938, c_loss=0.0262\n",
            "Epoch 1886/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6974, c_loss=0.0262\n",
            "Epoch 1887/2000\n",
            "Step 0: d_loss=1.3830, g_loss=0.6931, c_loss=0.0261\n",
            "Epoch 1888/2000\n",
            "Step 0: d_loss=1.3831, g_loss=0.6979, c_loss=0.0261\n",
            "Epoch 1889/2000\n",
            "Step 0: d_loss=1.3833, g_loss=0.6928, c_loss=0.0261\n",
            "Epoch 1890/2000\n",
            "Step 0: d_loss=1.3838, g_loss=0.6971, c_loss=0.0260\n",
            "Epoch 1891/2000\n",
            "Step 0: d_loss=1.3832, g_loss=0.6934, c_loss=0.0260\n",
            "Epoch 1892/2000\n",
            "Step 0: d_loss=1.3837, g_loss=0.6968, c_loss=0.0260\n",
            "Epoch 1893/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6928, c_loss=0.0259\n",
            "Epoch 1894/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.6968, c_loss=0.0259\n",
            "Epoch 1895/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6927, c_loss=0.0259\n",
            "Epoch 1896/2000\n",
            "Step 0: d_loss=1.3845, g_loss=0.6967, c_loss=0.0259\n",
            "Epoch 1897/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6923, c_loss=0.0258\n",
            "Epoch 1898/2000\n",
            "Step 0: d_loss=1.3849, g_loss=0.6968, c_loss=0.0258\n",
            "Epoch 1899/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6911, c_loss=0.0258\n",
            "Epoch 1900/2000\n",
            "Step 0: d_loss=1.3854, g_loss=0.6974, c_loss=0.0258\n",
            "Epoch 1901/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.6899, c_loss=0.0257\n",
            "Epoch 1902/2000\n",
            "Step 0: d_loss=1.3859, g_loss=0.6981, c_loss=0.0257\n",
            "Epoch 1903/2000\n",
            "Step 0: d_loss=1.3863, g_loss=0.6889, c_loss=0.0257\n",
            "Epoch 1904/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6982, c_loss=0.0257\n",
            "Epoch 1905/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.6884, c_loss=0.0256\n",
            "Epoch 1906/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6975, c_loss=0.0256\n",
            "Epoch 1907/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.6884, c_loss=0.0256\n",
            "Epoch 1908/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6963, c_loss=0.0256\n",
            "Epoch 1909/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6889, c_loss=0.0255\n",
            "Epoch 1910/2000\n",
            "Step 0: d_loss=1.3886, g_loss=0.6958, c_loss=0.0255\n",
            "Epoch 1911/2000\n",
            "Step 0: d_loss=1.3890, g_loss=0.6887, c_loss=0.0255\n",
            "Epoch 1912/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.6954, c_loss=0.0255\n",
            "Epoch 1913/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6872, c_loss=0.0254\n",
            "Epoch 1914/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.6970, c_loss=0.0254\n",
            "Epoch 1915/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6853, c_loss=0.0254\n",
            "Epoch 1916/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6978, c_loss=0.0254\n",
            "Epoch 1917/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6847, c_loss=0.0254\n",
            "Epoch 1918/2000\n",
            "Step 0: d_loss=1.3908, g_loss=0.6976, c_loss=0.0253\n",
            "Epoch 1919/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6849, c_loss=0.0253\n",
            "Epoch 1920/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6971, c_loss=0.0253\n",
            "Epoch 1921/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6856, c_loss=0.0252\n",
            "Epoch 1922/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6963, c_loss=0.0252\n",
            "Epoch 1923/2000\n",
            "Step 0: d_loss=1.3911, g_loss=0.6857, c_loss=0.0252\n",
            "Epoch 1924/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6969, c_loss=0.0252\n",
            "Epoch 1925/2000\n",
            "Step 0: d_loss=1.3905, g_loss=0.6850, c_loss=0.0251\n",
            "Epoch 1926/2000\n",
            "Step 0: d_loss=1.3904, g_loss=0.6982, c_loss=0.0251\n",
            "Epoch 1927/2000\n",
            "Step 0: d_loss=1.3903, g_loss=0.6848, c_loss=0.0251\n",
            "Epoch 1928/2000\n",
            "Step 0: d_loss=1.3901, g_loss=0.6983, c_loss=0.0251\n",
            "Epoch 1929/2000\n",
            "Step 0: d_loss=1.3897, g_loss=0.6850, c_loss=0.0251\n",
            "Epoch 1930/2000\n",
            "Step 0: d_loss=1.3891, g_loss=0.6989, c_loss=0.0250\n",
            "Epoch 1931/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6853, c_loss=0.0250\n",
            "Epoch 1932/2000\n",
            "Step 0: d_loss=1.3884, g_loss=0.6997, c_loss=0.0250\n",
            "Epoch 1933/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6856, c_loss=0.0250\n",
            "Epoch 1934/2000\n",
            "Step 0: d_loss=1.3876, g_loss=0.7000, c_loss=0.0250\n",
            "Epoch 1935/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.6858, c_loss=0.0249\n",
            "Epoch 1936/2000\n",
            "Step 0: d_loss=1.3871, g_loss=0.7007, c_loss=0.0249\n",
            "Epoch 1937/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.6860, c_loss=0.0249\n",
            "Epoch 1938/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.7016, c_loss=0.0248\n",
            "Epoch 1939/2000\n",
            "Step 0: d_loss=1.3858, g_loss=0.6862, c_loss=0.0248\n",
            "Epoch 1940/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.7023, c_loss=0.0248\n",
            "Epoch 1941/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.6862, c_loss=0.0247\n",
            "Epoch 1942/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7029, c_loss=0.0247\n",
            "Epoch 1943/2000\n",
            "Step 0: d_loss=1.3848, g_loss=0.6859, c_loss=0.0247\n",
            "Epoch 1944/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.7035, c_loss=0.0247\n",
            "Epoch 1945/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6866, c_loss=0.0246\n",
            "Epoch 1946/2000\n",
            "Step 0: d_loss=1.3841, g_loss=0.7035, c_loss=0.0246\n",
            "Epoch 1947/2000\n",
            "Step 0: d_loss=1.3840, g_loss=0.6863, c_loss=0.0246\n",
            "Epoch 1948/2000\n",
            "Step 0: d_loss=1.3843, g_loss=0.7040, c_loss=0.0246\n",
            "Epoch 1949/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6857, c_loss=0.0246\n",
            "Epoch 1950/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.7047, c_loss=0.0245\n",
            "Epoch 1951/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6843, c_loss=0.0245\n",
            "Epoch 1952/2000\n",
            "Step 0: d_loss=1.3846, g_loss=0.7060, c_loss=0.0245\n",
            "Epoch 1953/2000\n",
            "Step 0: d_loss=1.3844, g_loss=0.6830, c_loss=0.0244\n",
            "Epoch 1954/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.7076, c_loss=0.0244\n",
            "Epoch 1955/2000\n",
            "Step 0: d_loss=1.3857, g_loss=0.6807, c_loss=0.0244\n",
            "Epoch 1956/2000\n",
            "Step 0: d_loss=1.3852, g_loss=0.7087, c_loss=0.0244\n",
            "Epoch 1957/2000\n",
            "Step 0: d_loss=1.3862, g_loss=0.6796, c_loss=0.0243\n",
            "Epoch 1958/2000\n",
            "Step 0: d_loss=1.3864, g_loss=0.7091, c_loss=0.0243\n",
            "Epoch 1959/2000\n",
            "Step 0: d_loss=1.3865, g_loss=0.6775, c_loss=0.0243\n",
            "Epoch 1960/2000\n",
            "Step 0: d_loss=1.3875, g_loss=0.7116, c_loss=0.0243\n",
            "Epoch 1961/2000\n",
            "Step 0: d_loss=1.3870, g_loss=0.6742, c_loss=0.0242\n",
            "Epoch 1962/2000\n",
            "Step 0: d_loss=1.3881, g_loss=0.7146, c_loss=0.0242\n",
            "Epoch 1963/2000\n",
            "Step 0: d_loss=1.3883, g_loss=0.6705, c_loss=0.0242\n",
            "Epoch 1964/2000\n",
            "Step 0: d_loss=1.3885, g_loss=0.7179, c_loss=0.0242\n",
            "Epoch 1965/2000\n",
            "Step 0: d_loss=1.3888, g_loss=0.6666, c_loss=0.0242\n",
            "Epoch 1966/2000\n",
            "Step 0: d_loss=1.3887, g_loss=0.7220, c_loss=0.0241\n",
            "Epoch 1967/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6625, c_loss=0.0241\n",
            "Epoch 1968/2000\n",
            "Step 0: d_loss=1.3898, g_loss=0.7259, c_loss=0.0241\n",
            "Epoch 1969/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.6579, c_loss=0.0241\n",
            "Epoch 1970/2000\n",
            "Step 0: d_loss=1.3899, g_loss=0.7308, c_loss=0.0240\n",
            "Epoch 1971/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6535, c_loss=0.0240\n",
            "Epoch 1972/2000\n",
            "Step 0: d_loss=1.3909, g_loss=0.7354, c_loss=0.0240\n",
            "Epoch 1973/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6488, c_loss=0.0240\n",
            "Epoch 1974/2000\n",
            "Step 0: d_loss=1.3915, g_loss=0.7396, c_loss=0.0239\n",
            "Epoch 1975/2000\n",
            "Step 0: d_loss=1.3914, g_loss=0.6463, c_loss=0.0239\n",
            "Epoch 1976/2000\n",
            "Step 0: d_loss=1.3916, g_loss=0.7405, c_loss=0.0239\n",
            "Epoch 1977/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.6478, c_loss=0.0239\n",
            "Epoch 1978/2000\n",
            "Step 0: d_loss=1.3910, g_loss=0.7372, c_loss=0.0238\n",
            "Epoch 1979/2000\n",
            "Step 0: d_loss=1.3906, g_loss=0.6530, c_loss=0.0238\n",
            "Epoch 1980/2000\n",
            "Step 0: d_loss=1.3900, g_loss=0.7300, c_loss=0.0238\n",
            "Epoch 1981/2000\n",
            "Step 0: d_loss=1.3894, g_loss=0.6613, c_loss=0.0238\n",
            "Epoch 1982/2000\n",
            "Step 0: d_loss=1.3889, g_loss=0.7212, c_loss=0.0238\n",
            "Epoch 1983/2000\n",
            "Step 0: d_loss=1.3882, g_loss=0.6700, c_loss=0.0237\n",
            "Epoch 1984/2000\n",
            "Step 0: d_loss=1.3879, g_loss=0.7132, c_loss=0.0237\n",
            "Epoch 1985/2000\n",
            "Step 0: d_loss=1.3873, g_loss=0.6777, c_loss=0.0237\n",
            "Epoch 1986/2000\n",
            "Step 0: d_loss=1.3869, g_loss=0.7069, c_loss=0.0237\n",
            "Epoch 1987/2000\n",
            "Step 0: d_loss=1.3867, g_loss=0.6832, c_loss=0.0236\n",
            "Epoch 1988/2000\n",
            "Step 0: d_loss=1.3860, g_loss=0.7028, c_loss=0.0236\n",
            "Epoch 1989/2000\n",
            "Step 0: d_loss=1.3856, g_loss=0.6876, c_loss=0.0236\n",
            "Epoch 1990/2000\n",
            "Step 0: d_loss=1.3855, g_loss=0.6995, c_loss=0.0236\n",
            "Epoch 1991/2000\n",
            "Step 0: d_loss=1.3850, g_loss=0.6906, c_loss=0.0235\n",
            "Epoch 1992/2000\n",
            "Step 0: d_loss=1.3847, g_loss=0.6981, c_loss=0.0235\n",
            "Epoch 1993/2000\n",
            "Step 0: d_loss=1.3842, g_loss=0.6923, c_loss=0.0235\n",
            "Epoch 1994/2000\n",
            "Step 0: d_loss=1.3839, g_loss=0.6974, c_loss=0.0235\n",
            "Epoch 1995/2000\n",
            "Step 0: d_loss=1.3836, g_loss=0.6933, c_loss=0.0235\n",
            "Epoch 1996/2000\n",
            "Step 0: d_loss=1.3834, g_loss=0.6972, c_loss=0.0234\n",
            "Epoch 1997/2000\n",
            "Step 0: d_loss=1.3829, g_loss=0.6943, c_loss=0.0234\n",
            "Epoch 1998/2000\n",
            "Step 0: d_loss=1.3828, g_loss=0.6968, c_loss=0.0234\n",
            "Epoch 1999/2000\n",
            "Step 0: d_loss=1.3823, g_loss=0.6953, c_loss=0.0234\n",
            "Epoch 2000/2000\n",
            "Step 0: d_loss=1.3820, g_loss=0.6968, c_loss=0.0233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the classifier on the test dataset\n",
        "#test_loss, test_accuracy = cr.evaluate(test_dataset)\n",
        "#print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "y_pred_probs = cr.predict(test_dataset)\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_labels = tf.argmax(y_pred_probs, axis=1).numpy()\n",
        "y_true_labels = tf.concat([y for _, y in test_dataset], axis=0).numpy()\n",
        "\n",
        "# If the labels are one-hot encoded, convert them to integers\n",
        "if y_true_labels.ndim > 1:\n",
        "    y_true_labels = tf.argmax(y_true_labels, axis=1).numpy()\n",
        "\n",
        "# Compute the confusion matrix\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_true_labels,y_pred_labels)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display.plot()\n",
        "pt.show()\n",
        "\n",
        "confusion_matrix_per_class = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1, keepdims=True) * 100\n",
        "cm_display_per_class = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_per_class,display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display_per_class.plot(cmap=\"Blues\", values_format=\".2f\")\n",
        "pt.title(\"Confusion Matrix (Percentage Per Class)\")\n",
        "pt.show()\n",
        "\n",
        "acc_test=accuracy_score(y_true_labels,y_pred_labels)\n",
        "print(\"Accuracy Test: \", acc_test*100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "VXb9JkeY7p1p",
        "outputId": "a67cfc11-2252-4075-8934-bd00134d5cd4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  97.42857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "gH_uu1omYuqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.1, random_state=42)\n",
        "\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_labeled_pca = pca.transform(X_labeled)\n",
        "X_unlabeled_pca = pca.transform(X_unlabeled)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "minmax.fit(X_train_pca)\n",
        "\n",
        "X_train_scaled = minmax.transform(X_train_pca).astype(np.float32)\n",
        "X_test_scaled = minmax.transform(X_test_pca).astype(np.float32)\n",
        "X_labeled_scaled = minmax.transform(X_labeled_pca).astype(np.float32)\n",
        "X_unlabeled_scaled = minmax.transform(X_unlabeled_pca).astype(np.float32)\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(y_labeled))\n",
        "y_labeled_onehot = tf.keras.utils.to_categorical(y_labeled, num_classes=num_classes)\n",
        "labels = y_labeled_onehot\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "labeled_dataset = tf.data.Dataset.from_tensor_slices((X_labeled_scaled, y_labeled_onehot))\n",
        "labeled_dataset = labeled_dataset.shuffle(buffer_size=len(X_labeled_scaled)).batch(batch_size)\n",
        "\n",
        "\n",
        "unlabeled_dataset = tf.data.Dataset.from_tensor_slices(X_unlabeled_scaled)\n",
        "unlabeled_dataset = unlabeled_dataset.shuffle(buffer_size=len(X_unlabeled_scaled)).batch(batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_test))\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_onehot))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "NK0rKWOVYuMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}